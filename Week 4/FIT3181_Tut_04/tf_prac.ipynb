{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import copy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.]\n",
      " [2.]]\n"
     ]
    }
   ],
   "source": [
    "# Optimal weights that we can obtain \n",
    "\n",
    "W_opt = np.array([[1], [2]], dtype=np.float64) \n",
    "print(W_opt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (500, 2)\n"
     ]
    }
   ],
   "source": [
    "N = 500 # Number of data points \n",
    "delta = 2 # some random scaler \n",
    "X1 = (np.random.rand(N) - 0.5) * 2 * delta \n",
    "X2 = (np.random.rand(N) - 0.5) * 2 * delta \n",
    "X_train = np.array([x for x in zip(X1,X2)])\n",
    "print(f\"X_train shape: {X_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive labels: 249, number of negative labels: 251\n"
     ]
    }
   ],
   "source": [
    "y_value = X_train.dot(W_opt) + np.random.normal(0.0, 0.2)\n",
    "y_train = np.ones([N, 1])\n",
    "y_train[np.where(y_value < 0)] = 0\n",
    "num_pos = len(np.argwhere(y_train == 1)[:, 0])\n",
    "num_neg = len(np.argwhere(y_train == 0)[:, 0])\n",
    "print(\"Number of positive labels: {}, number of negative labels: {}\".format(num_pos, num_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "mnist = tf.keras.datasets.mnist \n",
    "(X_train_full_img, y_train_full), (X_test_img, y_test) = mnist.load_data() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784) (60000,)\n",
      "(10000, 784) (10000,)\n"
     ]
    }
   ],
   "source": [
    "num_train = X_train_full_img.shape[0] # First value tells you the number of ds \n",
    "num_test = X_test_img.shape[0] \n",
    "X_train_full = X_train_full_img.reshape(num_train, -1)/255.0\n",
    "X_test = X_test_img.reshape(num_test, -1) / 255.0 \n",
    "print(X_train_full.shape, y_train_full.shape) \n",
    "print(X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN:\n",
    "    def __init__(self, n_classes=10, optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),\n",
    "                 batch_size=32, epochs=20, alpha=1.0, beta=1.0):\n",
    "        self.n_classes = 10\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.optimizer = optimizer\n",
    "        self.alpha = alpha  # hyper-parameter corresponding to entropy loss\n",
    "        self.beta = beta  # hyper-parameter for the max-margin loss\n",
    "\n",
    "        # create a tensorflow dataset and shuffle\n",
    "        self.train_full_set = tf.data.Dataset.from_tensor_slices(\n",
    "            (X_train_full, y_train_full)).shuffle(1000)\n",
    "\n",
    "        # TODO: I AM UNSURE WHY THIS DOESN'T WORK\n",
    "        self.test_full_set = tf.data.Dataset.from_tensor_slices(\n",
    "            (X_test, y_test))\n",
    "\n",
    "        # take train and valid sets from full dataset\n",
    "        self.train_set = self.train_full_set.take(50000)\n",
    "        self.valid_set = self.train_full_set.skip(50000).take(10000)\n",
    "        # batching train and valid sets\n",
    "        self.train_set = self.train_set.batch(self.batch_size).prefetch(1)\n",
    "        self.valid_set = self.valid_set.batch(self.batch_size).prefetch(1)\n",
    "\n",
    "        tf.keras.backend.set_floatx('float64')\n",
    "\n",
    "    def build(self):\n",
    "        self.model = Sequential([Dense(20, activation='relu'), Dense(20, activation='relu'),\n",
    "                                 Dense(self.n_classes, activation='softmax')])\n",
    "        self.model.compile(optimizer='SGD', loss=tf.keras.metrics.SparseCategoricalAccuracy())\n",
    "\n",
    "    def compute_loss(self, X, y):  # X is data batch, y is label batch\n",
    "        pred_probs = self.model(X)\n",
    "        l1 = tf.keras.losses.sparse_categorical_crossentropy(\n",
    "            y, pred_probs)  # Cross entropy loss\n",
    "        # Prediction entropy loss\n",
    "        l2 = tf.reduce_sum(- pred_probs * tf.math.log(pred_probs), axis=-1)\n",
    "\n",
    "        assert (l1.shape == l2.shape)\n",
    "        return l1 + self.alpha * l2\n",
    "\n",
    "    def compute_grads(self, X, y):\n",
    "        with tf.GradientTape() as g:  # use gradient tape to compute gradients\n",
    "            loss = self.compute_loss(X, y)\n",
    "        # compute gradients w.r.t. all trainable variables\n",
    "        grads = g.gradient(loss, self.model.trainable_variables)\n",
    "        return grads\n",
    "\n",
    "    def train_one_batch(self, X, y):  # train in one batch\n",
    "        grads = self.compute_grads(X, y)\n",
    "        # the gradients will be applied according to optimizer for example SGD, Adam, and etc.\n",
    "        self.optimizer.apply_gradients(\n",
    "            zip(grads, self.model.trainable_variables))\n",
    "\n",
    "    def evaluate(self, tf_dataset=None):\n",
    "        dataset_loss = tf.keras.metrics.Mean()\n",
    "        dataset_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "        for X, y in tf_dataset:\n",
    "            loss = self.compute_loss(X, y)\n",
    "            dataset_loss.update_state(loss)\n",
    "            dataset_accuracy.update_state(y, self.model(X, training=False))\n",
    "        return dataset_loss.result(), dataset_accuracy.result()\n",
    "\n",
    "    def save_model(self, path:str):\n",
    "        self.model.save(path)\n",
    "    \n",
    "    def train_or_pred(self, training=True):\n",
    "\n",
    "        if not training:\n",
    "            # X, y = self.test_full_set\n",
    "            dataset_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "            pred = self.model(X_test, training=False)\n",
    "            dataset_accuracy.update_state(y_test, pred)\n",
    "            print(f\"Test Accuracy: {dataset_accuracy.result():.4f}\")\n",
    "\n",
    "        else:\n",
    "            for epoch in range(self.epochs):\n",
    "                for X, y in self.train_set:  # use batch_index if you want to display something in iterations\n",
    "                    self.train_one_batch(X, y)\n",
    "                train_loss, train_acc = self.evaluate(self.train_set)\n",
    "                valid_loss, valid_acc = self.evaluate(self.valid_set)\n",
    "                print('Epoch {}: train acc={:.4f}, train loss={:.4f} | valid acc={:.4f}, valid loss= {:.4f}'.format(epoch + 1,\n",
    "                                                                                                                    train_acc, train_loss,\n",
    "                                                                                                                    valid_acc, valid_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.SGD(learning_rate=0.001)\n",
    "dnn = DNN(optimizer=opt, epochs=10, batch_size=64)\n",
    "dnn.build()\n",
    "# dnn.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train acc=0.9140, train loss=0.4867 | valid acc=0.9200, valid loss= 0.4450\n",
      "Epoch 2: train acc=0.9353, train loss=0.3664 | valid acc=0.9396, valid loss= 0.3441\n",
      "Epoch 3: train acc=0.9422, train loss=0.3293 | valid acc=0.9442, valid loss= 0.3134\n",
      "Epoch 4: train acc=0.9476, train loss=0.2883 | valid acc=0.9491, valid loss= 0.2838\n",
      "Epoch 5: train acc=0.9535, train loss=0.2587 | valid acc=0.9528, valid loss= 0.2667\n",
      "Epoch 6: train acc=0.9541, train loss=0.2459 | valid acc=0.9538, valid loss= 0.2612\n",
      "Epoch 7: train acc=0.9604, train loss=0.2176 | valid acc=0.9569, valid loss= 0.2410\n",
      "Epoch 8: train acc=0.9610, train loss=0.2147 | valid acc=0.9561, valid loss= 0.2458\n",
      "Epoch 9: train acc=0.9616, train loss=0.2140 | valid acc=0.9576, valid loss= 0.2432\n",
      "Epoch 10: train acc=0.9622, train loss=0.2027 | valid acc=0.9568, valid loss= 0.2314\n",
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000002839BAEEE50> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('signature_function', 'signature_key'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x000002839BAEEE50> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('signature_function', 'signature_key'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "INFO:tensorflow:Assets written to: mymodels/exe3dnn\\assets\n",
      "Test Accuracy: 0.9542\n"
     ]
    }
   ],
   "source": [
    "dnn.train_or_pred()\n",
    "dnn.save_model(\"mymodels/exe3dnn\")\n",
    "dnn.train_or_pred(training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = tf.keras.models.load_model('mymodels/exe3dnn') \n",
    "# Probs not a good idea to save a DNN class\n",
    "new_model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b916ba2c14536b214077dfec6c206136f3cd9dd298193f9fd8924ac064eebdc3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
