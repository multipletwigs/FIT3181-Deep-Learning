{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#0b486b\">  FIT3181: Deep Learning (2022)</span>\n",
    "***\n",
    "*CE/Lecturer:* Dr **Trung Le** | trunglm@monash.edu <br/>\n",
    "*Head Tutor:* Mr **Thanh Nguyen** | thanh.nguyen4@monash.edu <br/>\n",
    "<br/>\n",
    "Department of Data Science and AI, Faculty of Information Technology, Monash University, Australia\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#0b486b\">  Student Information</span>\n",
    "Surname: **Khong**  <br/>\n",
    "Firstname: **Lap Hoe**    <br/>\n",
    "Student ID: **32114818**    <br/>\n",
    "Email: **lkho0007@student.monash.edu**    <br/>\n",
    "Your tutorial time: **Friday 2PM**    <br/>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#0b486b\">Assignment 02: Neural Embedding and Sequence Modelling</span>\n",
    "### Due: <span style=\"color:red\">11:59pm 23 October 2022</span>  (Sunday)\n",
    "\n",
    "#### <span style=\"color:red\">Important note:</span> This is an **individual** assignment. It contributes **20%** to you final mark. Read the assignment instruction carefully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#0b486b\">Instructions</span>\n",
    "\n",
    "This notebook has been prepared for your to complete Assignment 2. The theme of this assignment is about practical machine learning knowledge and skills in deep neural networks, word embedding and text analytics. Some sections have been partially completed to help you get\n",
    "started. **The total marks for this notebook is 100**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Before you start, read the entire notebook carefully once to understand what you need to do. <br><br>\n",
    "* For each cell marked with **#YOU ARE REQUIRED TO INSERT YOUR CODES IN THIS CELL**, there will be places where you **must** supply your own codes when instructed. <br>\n",
    "\n",
    "This assignment contains **five** parts:\n",
    "\n",
    "* Part 1: Questions on downloading and preprocessing data **[5 points]**\n",
    "* Part 2: Questions on using Word2Vect to transform texts to vectors **[20 points]**\n",
    "* Part 3: Coding assessment on Text CNN for sequence modeling and neural embedding **[10 points]**\n",
    "* Part 4: Coding assessment on BERT for a feature extraction **[10 points]**\n",
    "* Part 5: Coding assessment on RNNs for sequence modeling and neural embedding **[55 points]**\n",
    "\n",
    "\n",
    "**Hint**: This assignment was essentially designed based on the lectures and tutorials sessions covered from Weeks 7 to 10. You are strongly recommended to go through these contents thoroughly which might help you to complete this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#0b486b\">What to submit</span>\n",
    "\n",
    "This assignment is to be completed individually and submitted to Moodle unit site. **By the due date, you are required to submit one  <span style=\"color:red; font-weight:bold\">single zip file, named xxx_assignment02_solution.zip</span> where `xxx` is your student ID, to the corresponding Assignment (Dropbox) in Moodle**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***For example, if your student ID is <span style=\"color:red; font-weight:bold\">12356</span>, then gather all of your assignment solution to folder, create a zip file named <span style=\"color:red; font-weight:bold\">123456_assignment02_solution.zip</span> and submit this file.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within this zip folder, you **must** submit the following files:\n",
    "1.\t**Assignment02_solution.ipynb**:  this is your Python notebook solution source file.\n",
    "1.\t**Assignment02_output.html**: this is the output of your Python notebook solution *exported* in html format.\n",
    "1.\tAny **extra files or folder** needed to complete your assignment (e.g., images used in your answers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#0b486b\">Set random seeds</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with importing tensorflow and numpy and setting random seeds for TF and numpy. You can use any seeds you prefer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.random.set_seed(6789)\n",
    "np.random.seed(6789)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#0b486b\">Part 1: Download and preprocess the data</span>\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red; font-weight:bold\">[Total marks for this part: 5 points]<span></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset we use for this assignment is a question classification dataset for which the train set consists of $5,500$ questions belonging to 6 coarse question categories including:\n",
    "- abbreviation (ABBR), \n",
    "- entity (ENTY), \n",
    "- description (DESC), \n",
    "- human (HUM), \n",
    "- location (LOC) and \n",
    "- numeric (NUM).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing data is an inital and important step in any machine learning or deep learning projects. The following *DataManager* class helps you to download data and preprocess data for the later questions of this assignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import collections\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "class DataManager:\n",
    "    def __init__(self, verbose=True, maxlen= 50, random_state=6789):\n",
    "        self.verbose = verbose\n",
    "        self.max_sentence_len = 0\n",
    "        self.str_questions = list()\n",
    "        self.str_labels = list()\n",
    "        self.numeral_labels = list()\n",
    "        self.maxlen = maxlen\n",
    "        self.numeral_data = list()\n",
    "        self.random_state = random_state\n",
    "        self.random = np.random.RandomState(random_state)\n",
    "        \n",
    "    @staticmethod\n",
    "    def maybe_download(dir_name, file_name, url, verbose= True):\n",
    "        if not os.path.exists(dir_name):\n",
    "            os.mkdir(dir_name)\n",
    "        if not os.path.exists(os.path.join(dir_name, file_name)):\n",
    "            urlretrieve(url + file_name, os.path.join(dir_name, file_name))\n",
    "        if verbose:\n",
    "            print(\"Downloaded successfully {}\".format(file_name))\n",
    "    \n",
    "    def read_data(self, dir_name, file_names):\n",
    "        self.str_questions= list(); self.str_labels= list()\n",
    "        for file_name in file_names:\n",
    "            file_path= os.path.join(dir_name, file_name)\n",
    "            with open(file_path, \"r\", encoding=\"latin-1\") as f:\n",
    "                for row in f:\n",
    "                    row_str= row.split(\":\")\n",
    "                    label, question= row_str[0], row_str[1]\n",
    "                    question= question.lower()\n",
    "                    self.str_labels.append(label)\n",
    "                    self.str_questions.append(question[0:-1])\n",
    "                    if self.max_sentence_len < len(self.str_questions[-1]):\n",
    "                        self.max_sentence_len= len(self.str_questions[-1])\n",
    "         \n",
    "        # turns labels into numbers\n",
    "        le= preprocessing.LabelEncoder()\n",
    "        le.fit(self.str_labels)\n",
    "        self.numeral_labels = np.array(le.transform(self.str_labels))\n",
    "        self.str_classes= le.classes_\n",
    "        self.num_classes= len(self.str_classes)\n",
    "        if self.verbose:\n",
    "            print(\"\\nSample questions... \\n\")\n",
    "            print(self.str_questions[0:5])\n",
    "            print(\"Labels {}\\n\\n\".format(self.str_classes))\n",
    "    \n",
    "    def manipulate_data(self):\n",
    "        tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "        tokenizer.fit_on_texts(self.str_questions)\n",
    "        self.numeral_data = tokenizer.texts_to_sequences(self.str_questions)\n",
    "        self.numeral_data = tf.keras.preprocessing.sequence.pad_sequences(self.numeral_data, padding='post', truncating= 'post', maxlen= self.maxlen)\n",
    "        self.word2idx = tokenizer.word_index\n",
    "        self.word2idx = {k:v for k,v in self.word2idx.items()}\n",
    "        self.idx2word = {v:k for k,v in self.word2idx.items()}\n",
    "        self.vocab_size = len(self.word2idx)\n",
    "    \n",
    "    def train_valid_split(self, train_ratio=0.9):\n",
    "        idxs = np.random.permutation(np.arange(len(self.str_questions)))\n",
    "        train_size = int(train_ratio*len(idxs)) +1\n",
    "        self.train_str_questions, self.valid_str_questions = self.str_questions[0:train_size], self.str_questions[train_size:]\n",
    "        self.train_numeral_data, self.valid_numeral_data = self.numeral_data[0:train_size], self.numeral_data[train_size:]\n",
    "        self.train_numeral_labels, self.valid_numeral_labels = self.numeral_labels[0:train_size], self.numeral_labels[train_size:]\n",
    "        self.tf_train_set = tf.data.Dataset.from_tensor_slices((self.train_numeral_data, self.train_numeral_labels))\n",
    "        self.tf_valid_set = tf.data.Dataset.from_tensor_slices((self.valid_numeral_data, self.valid_numeral_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Downloaded successfully train_1000.label\n",
      "Downloaded successfully TREC_10.label\n",
      "\n",
      "Sample questions... \n",
      "\n",
      "['manner how did serfdom develop in and then leave russia ?', 'cremat what films featured the character popeye doyle ?', \"manner how can i find a list of celebrities ' real names ?\", 'animal what fowl grabs the spotlight after the chinese year of the monkey ?', 'exp what is the full form of .com ?']\n",
      "Labels ['ABBR' 'DESC' 'ENTY' 'HUM' 'LOC' 'NUM']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Loading data...')\n",
    "DataManager.maybe_download(\"Data\", \"train_1000.label\", \"http://cogcomp.org/Data/QA/QC/\")\n",
    "DataManager.maybe_download(\"Data\", \"TREC_10.label\", \"http://cogcomp.org/Data/QA/QC/\")\n",
    "\n",
    "dm = DataManager(maxlen=100)\n",
    "dm.read_data(\"Data/\", [\"train_1000.label\", \"TREC_10.label\"])   # read data\n",
    "# If you want to play around with a bigger dataset, you can try train_set.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.manipulate_data()\n",
    "dm.train_valid_split(train_ratio=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You now have a data manager, named *dm* containing the training and validiation sets in both text and numeric forms. Your task is to play around and read this code to figure out the meanings of some important attributes that will be used in the next parts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">**Question 1.1**</span> \n",
    "**What is the purpose of `self.train_str_questions` and `self.train_numeral_labels`? Write your code to print out the first five questions with labels in the training set.**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[1 point]</span></div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of `self.train_str_questions` is to store the actual string representation of the dataset. This was created by splitting the dataset into train test valid portions, and this represents the training portion. \n",
    "\n",
    "The purpose `self.train_numeral_labels` is to obtain the numeric representation of the labels. The original labels given were in text form. Having it as numeric categories can simplify the loss calculation process. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question number 1: \n",
      "manner how did serfdom develop in and then leave russia ? \n",
      "with index label 1, which is string label DESC\n",
      "\n",
      "Question number 2: \n",
      "cremat what films featured the character popeye doyle ? \n",
      "with index label 2, which is string label ENTY\n",
      "\n",
      "Question number 3: \n",
      "manner how can i find a list of celebrities ' real names ? \n",
      "with index label 1, which is string label DESC\n",
      "\n",
      "Question number 4: \n",
      "animal what fowl grabs the spotlight after the chinese year of the monkey ? \n",
      "with index label 2, which is string label ENTY\n",
      "\n",
      "Question number 5: \n",
      "exp what is the full form of .com ? \n",
      "with index label 0, which is string label ABBR\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Your code here\n",
    "five_questions = dm.train_str_questions[:5] \n",
    "five_labels_idx = dm.train_numeral_labels[:5] \n",
    "five_labels_text = dm.str_labels[:5]\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"Question number {i + 1}: \\n{five_questions[i]} \\nwith index label {five_labels_idx[i]}, which is string label {five_labels_text[i]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">**Question 1.2**</span> \n",
    "**What is the purpose of `self.train_numeral_data`? Write your code to print out the first five questions in the numeric format with labels in the training set.**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[1 point]</span></div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of `self.train_numeral_data` is ... watch the lecture slides. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question number 1: \n",
      "[  35   11   18 1030 1031    5   25  561 1032  562    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0] \n",
      "with index label 1, which is string label DESC\n",
      "\n",
      "Question number 2: \n",
      "[  43    2  563 1033    1  164 1034 1035    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0] \n",
      "with index label 2, which is string label ENTY\n",
      "\n",
      "Question number 3: \n",
      "[  35   11   45   40   60    7  389    4 1036   88  295  165    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0] \n",
      "with index label 1, which is string label DESC\n",
      "\n",
      "Question number 4: \n",
      "[  34    2 1037 1038    1 1039  232    1  564   50    4    1 1040    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0] \n",
      "with index label 2, which is string label ENTY\n",
      "\n",
      "Question number 5: \n",
      "[ 61   2   3   1 390 233   4 199   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0] \n",
      "with index label 0, which is string label ABBR\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Your code here\n",
    "#Your code here\n",
    "five_questions = dm.train_numeral_data[:5] \n",
    "five_labels_idx = dm.train_numeral_labels[:5] \n",
    "five_labels_text = dm.str_labels[:5]\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"Question number {i + 1}: \\n{five_questions[i]} \\nwith index label {five_labels_idx[i]}, which is string label {five_labels_text[i]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">**Question 1.3**</span> \n",
    "**What is the purpose of two dictionaries: `self.word2idx` and `self.idx2word`? Write your code to print out the first five key-value pairs of those dictionaries.**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[1 point]</span></div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of `self.word2idx` is ... \n",
    "\n",
    "the purpose of `self.idx2word` is ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key-value pair 1: Key is the and the value is 1\n",
      "Key-value pair 2: Key is what and the value is 2\n",
      "Key-value pair 3: Key is is and the value is 3\n",
      "Key-value pair 4: Key is of and the value is 4\n",
      "Key-value pair 5: Key is in and the value is 5\n"
     ]
    }
   ],
   "source": [
    "for idx, key in enumerate(dm.word2idx):\n",
    "    if idx == 5:\n",
    "        break \n",
    "    else:\n",
    "        print(f\"Key-value pair {idx + 1}: Key is {key} and the value is {dm.word2idx[key]}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key-value pair 1: Key is 1 and the value is the\n",
      "Key-value pair 2: Key is 2 and the value is what\n",
      "Key-value pair 3: Key is 3 and the value is is\n",
      "Key-value pair 4: Key is 4 and the value is of\n",
      "Key-value pair 5: Key is 5 and the value is in\n"
     ]
    }
   ],
   "source": [
    "for idx, key in enumerate(dm.idx2word):\n",
    "    if idx == 5:\n",
    "        break \n",
    "    else:\n",
    "        print(f\"Key-value pair {idx + 1}: Key is {key} and the value is {dm.idx2word[key]}\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">**Question 1.4**</span> \n",
    "**What is the purpose of `self.tf_train_set`? Write your code to print out the first five items of `self.tf_train_set`.**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[1 point]</span></div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of `self.tf_train_set` is ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,) 1\n",
      "(100,) 2\n",
      "(100,) 1\n",
      "(100,) 2\n",
      "(100,) 0\n"
     ]
    }
   ],
   "source": [
    "#Your code here\n",
    "tf_train_iter = dm.tf_train_set.as_numpy_iterator() \n",
    "for idx, (train_x, train_y) in enumerate(tf_train_iter):\n",
    "    if idx == 5:\n",
    "        break\n",
    "    print(train_x.shape, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">**Question 1.5**</span> \n",
    "**What is the purpose of `self.tf_valid_set`? Write your code to print out the first five items of `self.tf_valid_set`.**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[1 point]</span></div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Your answer here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  23    2   23   13  243    3   53    1 3031   20    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0] 4\n",
      "[   6   12   98    1 3032    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0] 3\n",
      "[  34    2   14    1  248   49  648  802   74 3033   53    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0] 2\n",
      "[   8    2    3    1  557  210    4 3034    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0] 5\n",
      "[   8   22    3    1 1004 3035 3036  193    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0] 4\n"
     ]
    }
   ],
   "source": [
    "#Your code here\n",
    "tf_valid_iter = dm.tf_valid_set.as_numpy_iterator() \n",
    "for idx, (valid_x, valid_y) in enumerate(tf_valid_iter):\n",
    "    if idx == 5:\n",
    "        break\n",
    "    print(valid_x, valid_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#0b486b\">Part 2: Using Word2Vect to transform texts to vectors </span>\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red; font-weight:bold\">[Total marks for this part: 20 points]<span></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, you will be assessed on how to use a pretrained Word2Vect model for realizing a machine learning task. Basically, you will use this pretrained Word2Vect to transform the questions in the above dataset stored in the *data manager object dm* to numeric form for training a Support Vector Machine in sckit-learn.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\us\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (4.2.0)\n",
      "Requirement already satisfied: Cython==0.29.28 in c:\\users\\us\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from gensim) (0.29.28)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\us\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from gensim) (1.5.4)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\us\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from gensim) (1.23.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\us\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from gensim) (6.2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -jango (c:\\users\\us\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -jango (c:\\users\\us\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -jango (c:\\users\\us\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -jango (c:\\users\\us\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -jango (c:\\users\\us\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -jango (c:\\users\\us\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip available: 22.2.1 -> 22.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">**Question 2.1**</span> \n",
    "**Write code to download the pretrained model *glove-wiki-gigaword-100*. Note that this model transforms a word in its dictionary to a $100$ dimensional vector.**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[2 points]</span></div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vect = api.load('glove-wiki-gigaword-100')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">**Question 2.2**</span> \n",
    "\n",
    "**Write code for the function *get_word_vector(word, model)* used to transform a word to a vector using the pretrained Word2Vect model *model*. Note that for a word not in the vocabulary of our *word2vect*, you need to return a vector $0$ with 100 dimensions.**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[3 points]</span></div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_vector(word, model):\n",
    "    try:\n",
    "        vector = model.get_vector(word)\n",
    "    except: #word not in the vocabulary\n",
    "        vector = np.zeros(100, dtype=np.float32)\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">**Question 2.3**</span> \n",
    "\n",
    "**Write the code for the function `get_sentence_vector(sentence, important_score=None, model= None)`. Note that this function will transform a sentence to a 100-dimensional vector using the pretrained model *model*. In addition, the list *important_score* which has the same length as the *sentence* specifies the important scores of the words in the sentence. In your code, you first need to apply *softmax* function over *important_score* to obtain the important weight *important_weight* which forms a probability over the words of the sentence. Furthermore, the final vector of the sentence will be weighted sum of the individual vectors for words and the weights in *important_weight*.**\n",
    "- $important\\_weight = softmax(important\\_score)$.\n",
    "- $final\\_vector= important\\_weight[1]\\times v[1] + important\\_weight[2]\\times v[2] + ...+ important\\_weight[L]\\times v[L]$ where $L$ is the length of the sentence and $v[i]$ is the vector representation of the $i-th$  word in this sentence.\n",
    "\n",
    "**Note that if `important_score=None` is set by default, your function should return the average of all representation vectors corresponding to set `important_score=[1,1,...,1]`.**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[5 points]</span></div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_vector(sentence: str, important_score=None, model= None):\n",
    "    sentence_list = sentence.split(\" \") \n",
    "    sentence_vect = np.array([get_word_vector(word, model) for word in sentence_list])\n",
    "    important_weight = np.array([[1] for word in sentence_list])\n",
    "    \n",
    "    if important_score is not None:\n",
    "        important_weight = tf.nn.softmax(important_score) \n",
    "    return np.sum(sentence_vect * important_weight, axis=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">**Question 2.4**</span> \n",
    "\n",
    "**Write code to transform the training questions in *dm.train_str_questions* to feature vectors. Note that after running the following cell, you must have $X\\_train$ which is an numpy array of the feature vectors and $y\\_train$ which is an array of numeric labels (*Hint: dm.train_numeral_labels*). You can add more lines to the following cell if necessary. In addition, you should decide the *important_score* by yourself. For example, you might reckon that the 1st score is 1, the 2nd score is decayed by 0.9, the 3rd is decayed by 0.9, and so on.**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[2 points]</span></div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_importance(init, sentence_list, decay_rate=0.9, model=None):\n",
    "    return [[init * (decay_rate**i)] for word in sentence_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform training set to feature vectors...\n"
     ]
    }
   ],
   "source": [
    "print(\"Transform training set to feature vectors...\")\n",
    "\n",
    "X_train= [get_sentence_vector(ques, important_score=generate_importance(1, ques.split(), model=word2vect), model=word2vect) for ques in dm.train_str_questions]\n",
    "y_train= dm.train_numeral_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple assert that the final output of X_train is exactly the dimensions that we want\n",
    "# (1201, 100), which is (num_sentences, feature_vec_len)\n",
    "assert(np.shape(X_train) == (len(dm.train_str_questions), 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">**Question 2.5**</span> \n",
    "\n",
    "**Write code to transform the training questions in *dm.valid_str_questions* to feature vectors. Note that after running the following cell, you must have $X\\_valid$ which is an numpy array of the feature vectors and $y\\_valid$ which is an array of numeric labels (*Hint: dm.valid_numeral_labels*). You can add more lines to the following cell if necessary. In addition, you should decide the *important_score* by yourself. For example, you might reckon that the 1st score is 1, the 2nd score is decayed by 0.9, the 3rd is decayed by 0.9, and so on.**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[2 points]</span></div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform valid set to feature vectors...\n"
     ]
    }
   ],
   "source": [
    "print(\"Transform valid set to feature vectors...\")\n",
    "X_valid= [get_sentence_vector(ques, important_score=generate_importance(1, ques.split(), model=word2vect), model=word2vect) for ques in dm.valid_str_questions]\n",
    "y_valid= dm.valid_numeral_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple assert that the final output of X_valid is exactly the dimensions that we want\n",
    "# (1201, 100), which is (num_sentences, feature_vec_len)\n",
    "assert(np.shape(X_valid) == (len(dm.valid_str_questions), 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">**Question 2.6**</span> \n",
    "\n",
    "**It is now to use *MinMaxScaler(feature_range=(-1,1))* in sckit-learn to scale both training and valid sets to the range $(-1,1)$.**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[2 points]</span></div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "scaler.fit(X_valid)\n",
    "X_valid = scaler.transform(X_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">**Question 2.7**</span> \n",
    "\n",
    "**Declare a support vector machine (the class *SVC*  in sckit-learn) with RBF kernel, $C=1$, $gamma= 2^{-3}$ and fit on the training set.**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[2 points]</span></div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(C=1, gamma=0.125)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(C=1, gamma=0.125)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(C=1, gamma=0.125)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm = SVC(C=1, kernel=\"rbf\", gamma=0.125)\n",
    "svm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">**Question 2.8**</span> \n",
    "\n",
    "**Finally, we use the trained *svm* to evaluate on the valid set $X\\_valid$.**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[2 points]</span></div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8729096989966555\n"
     ]
    }
   ],
   "source": [
    "y_valid_pred= svm.predict(X_valid)\n",
    "acc = svm.score(X_valid, y_valid)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3, 2, 2, 4, 3, 5, 1, 1, 5, 5, 4, 3, 1, 2, 1, 2, 5, 1, 1, 5, 1,\n",
       "       2, 2, 2, 5, 1, 4, 3, 5, 3, 1, 2, 2, 1, 4, 1, 2, 5, 4, 1, 1, 1, 1,\n",
       "       2, 1, 1, 1, 2, 3, 4, 2, 2, 1, 2, 5, 2, 1, 5, 3, 3, 5, 2, 5, 1, 1,\n",
       "       4, 5, 1, 2, 3, 1, 3, 1, 2, 1, 5, 2, 2, 2, 3, 0, 1, 4, 1, 1, 1, 1,\n",
       "       1, 4, 2, 5, 1, 1, 1, 2, 3, 2, 2, 0, 5, 1, 5, 5, 4, 3, 2, 3, 5, 4,\n",
       "       4, 5, 1, 4, 1, 3, 4, 2, 2, 1, 5, 0, 2, 5, 1, 1, 5, 5, 1, 1, 5, 2,\n",
       "       2, 2, 4, 1, 2, 1, 2, 5, 2, 5, 3, 5, 3, 2, 2, 5, 1, 5, 4, 2, 2, 1,\n",
       "       3, 5, 5, 1, 2, 1, 1, 3, 4, 1, 5, 3, 2, 1, 4, 2, 2, 1, 1, 2, 5, 2,\n",
       "       1, 0, 1, 3, 3, 2, 3, 3, 5, 1, 3, 1, 1, 3, 1, 2, 4, 1, 1, 1, 5, 2,\n",
       "       4, 3, 2, 5, 0, 5, 2, 1, 4, 5, 1, 3, 2, 4, 0, 5, 1, 1, 1, 3, 3, 5,\n",
       "       4, 3, 1, 4, 3, 1, 4, 1, 3, 1, 3, 4, 3, 1, 1, 1, 5, 0, 2, 3, 2, 4,\n",
       "       2, 3, 2, 4, 3, 5, 2, 2, 2, 2, 5, 1, 5, 2, 4, 1, 1, 1, 2, 4, 5, 4,\n",
       "       2, 1, 3, 1, 1, 1, 1, 2, 5, 2, 2, 1, 4, 2, 4, 2, 2, 2, 1, 4, 1, 3,\n",
       "       5, 2, 3, 1, 1, 2, 5, 5, 3, 4, 5, 2, 1])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_valid_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#0b486b\">Part 3: Text CNN for sequence modeling and neural embedding </span>\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red; font-weight:bold\">[Total marks for this part: 10 points]<span></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">**Question 3.1**</span> \n",
    "\n",
    "**In what follows, you are required to complete the code for Text CNN for sentence classification. The paper of Text CNN can be found at this [link](https://www.aclweb.org/anthology/D14-1181.pdf). Here is the description of the Text CNN that you need to construct.**\n",
    "- There are three attributes (properties or instance variables): *embed_size, state_size, data_manager*.\n",
    "  - `embed_size`: the dimension of the vector space for which the words are embedded to using the embedding matrix.\n",
    "  - `state_size`: the number of filters used in *Conv1D* (reference [here](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv1D)).\n",
    "  - `data_manager`: the data manager to store information of the dataset.\n",
    "- The detail of the computational process is as follows:\n",
    "  - Given input $x$, we embed $x$ using the embedding matrix to obtain an $3D$ tensor $[batch\\_size \\times vocab\\_size \\times embed\\_size]$ as $h$.\n",
    "  - We feed $h$ to three Convd 1D layers, each of which has $state\\_size$ filters, padding=same, activation= relu, and $kernel\\_size= 3, 5, 7$ respectively to obtain $h1, h2, h3$. Note that each $h1, h2, h3$ is a 3D tensor with the shape $[batch\\_size \\times output\\_size \\times state\\_size]$.\n",
    "  - We then apply *GlobalMaxPool1D()* (reference [here](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GlobalMaxPool1D)) over $h1, h2, h3$ to obtain 2D tensors stored in $h1, h2, h3$ again.\n",
    "  - We then concatenate three 2D tensors $h1, h2, h3$ to obtain $h$. Note that you need to specify the axis to concatenate.\n",
    "  - We finally build up one dense layer on the top of $h$ for classification.\n",
    "  \n",
    "  <div style=\"text-align: right\"><span style=\"color:red\">[8 points]</span></div>\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN:\n",
    "    def __init__(self, embed_size= 128, state_size=16, data_manager=None):\n",
    "        self.data_manager = data_manager\n",
    "        self.embed_size = embed_size\n",
    "        self.state_size = state_size\n",
    "    \n",
    "    def build(self):\n",
    "        x = tf.keras.layers.Input(shape=[None])\n",
    "        h = tf.keras.layers.Embedding(self.data_manager.vocab_size +1, self.embed_size)(x)\n",
    "        h1 = tf.keras.layers.Conv1D(filters=self.state_size, kernel_size=3, padding=\"same\", activation=\"relu\")(h)\n",
    "        h2 = tf.keras.layers.Conv1D(filters=self.state_size, kernel_size=5, padding=\"same\", activation=\"relu\")(h)\n",
    "        h3 = tf.keras.layers.Conv1D(filters=self.state_size, kernel_size=7, padding=\"same\", activation=\"relu\")(h)\n",
    "        h1 = tf.keras.layers.GlobalMaxPool1D()(h1) \n",
    "        h2 = tf.keras.layers.GlobalMaxPool1D()(h2)\n",
    "        h3 = tf.keras.layers.GlobalMaxPool1D()(h3)\n",
    "        h = tf.concat([h1, h2, h3], 1)\n",
    "        h = tf.keras.layers.Dense(self.data_manager.num_classes, activation='softmax')(h)\n",
    "        self.model = tf.keras.Model(inputs=x, outputs=h) \n",
    "    \n",
    "    def compile_model(self, *args, **kwargs):\n",
    "        self.model.compile(*args, **kwargs)\n",
    "    \n",
    "    def fit(self, *args, **kwargs):\n",
    "        return self.model.fit(*args, **kwargs)\n",
    "    \n",
    "    def evaluate(self, *args, **kwargs):\n",
    "        self.model.evaluate(*args, **kwargs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">**Question 3.2**</span> \n",
    "**Here is the code to test TextCNN above. You can observe that TextCNN outperforms the traditional approach SVM + Word2Vect for this task. The reason is that TextCNN enables us to automatically learn the feature that fits to the task. This makes deep learning different from hand-crafted feature approaches. Complete the code to test the model. Note that when compiling the model, you can use the Adam optimizer.**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[2 points]</span></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "25/25 [==============================] - 2s 35ms/step - loss: 1.6574 - accuracy: 0.2773\n",
      "Epoch 2/20\n",
      "25/25 [==============================] - 1s 33ms/step - loss: 1.2997 - accuracy: 0.7094\n",
      "Epoch 3/20\n",
      "25/25 [==============================] - 1s 35ms/step - loss: 0.8513 - accuracy: 0.8410\n",
      "Epoch 4/20\n",
      "25/25 [==============================] - 1s 35ms/step - loss: 0.4650 - accuracy: 0.9209\n",
      "Epoch 5/20\n",
      "25/25 [==============================] - 1s 35ms/step - loss: 0.2511 - accuracy: 0.9642\n",
      "Epoch 6/20\n",
      "25/25 [==============================] - 1s 34ms/step - loss: 0.1483 - accuracy: 0.9750\n",
      "Epoch 7/20\n",
      "25/25 [==============================] - 1s 34ms/step - loss: 0.0920 - accuracy: 0.9900\n",
      "Epoch 8/20\n",
      "25/25 [==============================] - 1s 33ms/step - loss: 0.0590 - accuracy: 0.9975\n",
      "Epoch 9/20\n",
      "25/25 [==============================] - 1s 33ms/step - loss: 0.0394 - accuracy: 0.9992\n",
      "Epoch 10/20\n",
      "25/25 [==============================] - 1s 33ms/step - loss: 0.0275 - accuracy: 0.9992\n",
      "Epoch 11/20\n",
      "25/25 [==============================] - 1s 34ms/step - loss: 0.0199 - accuracy: 1.0000\n",
      "Epoch 12/20\n",
      "25/25 [==============================] - 1s 33ms/step - loss: 0.0150 - accuracy: 1.0000\n",
      "Epoch 13/20\n",
      "25/25 [==============================] - 1s 35ms/step - loss: 0.0116 - accuracy: 1.0000\n",
      "Epoch 14/20\n",
      "25/25 [==============================] - 1s 34ms/step - loss: 0.0093 - accuracy: 1.0000\n",
      "Epoch 15/20\n",
      "25/25 [==============================] - 1s 33ms/step - loss: 0.0076 - accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "25/25 [==============================] - 1s 35ms/step - loss: 0.0063 - accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "25/25 [==============================] - 1s 34ms/step - loss: 0.0053 - accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "25/25 [==============================] - 1s 35ms/step - loss: 0.0046 - accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "25/25 [==============================] - 1s 37ms/step - loss: 0.0040 - accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "25/25 [==============================] - 1s 34ms/step - loss: 0.0035 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x27f75868130>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_cnn = TextCNN(data_manager=dm)\n",
    "text_cnn.build()\n",
    "#Insert code here to compile the model\n",
    "text_cnn.compile_model(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "#Insert code here to train the model on 20 epochs\n",
    "text_cnn.fit(dm.tf_train_set.batch(50), epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#0b486b\">Part 4: Sentence representation with BERT </span>\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red; font-weight:bold\">[Total marks for this part: 10 points]<span></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">**Question 4.1**</span>\n",
    "**Use a pretrained BERT model to extract feaure vectors for the training and valid sets. You should choose an appropriate pretrained BERT model that fits your computational resource.**\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[6 points]</span></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "import tensorflow_text\n",
    "\n",
    "preprocessor_name = 'bert_en_uncased_preprocess/3'\n",
    "bert_model_name = 'small_bert/bert_en_uncased_L-2_H-128_A-2/2' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = hub.KerasLayer(\n",
    "    f\"https://tfhub.dev/tensorflow/{preprocessor_name}\", name=\"preprocessor\")\n",
    "\n",
    "encoder = hub.KerasLayer(\n",
    "    f\"https://tfhub.dev/tensorflow/{bert_model_name}\", name=\"encoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier_model(preprocessor,encoder):\n",
    "  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "  preprocessing_layer = preprocessor\n",
    "  encoder_inputs = preprocessing_layer(text_input)\n",
    "  encoder = encoder\n",
    "  outputs = encoder(encoder_inputs)\n",
    "  net = outputs['pooled_output']\n",
    "  net = tf.keras.layers.Dropout(0.1)(net)\n",
    "  net = tf.keras.layers.Dense(6, activation=None, name='classifier')(net)\n",
    "  return tf.keras.Model(text_input, net)\n",
    "\n",
    "\n",
    "classifier_model = build_classifier_model(preprocessor, encoder)\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "classifier_model.compile(optimizer='adam',\n",
    "              loss=loss_fn,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "38/38 [==============================] - 4s 52ms/step - loss: 1.9567 - accuracy: 0.1932\n",
      "Epoch 2/20\n",
      "38/38 [==============================] - 2s 51ms/step - loss: 1.6741 - accuracy: 0.2415\n",
      "Epoch 3/20\n",
      "38/38 [==============================] - 2s 50ms/step - loss: 1.5725 - accuracy: 0.3405\n",
      "Epoch 4/20\n",
      "38/38 [==============================] - 2s 51ms/step - loss: 1.5387 - accuracy: 0.3547\n",
      "Epoch 5/20\n",
      "38/38 [==============================] - 2s 52ms/step - loss: 1.4883 - accuracy: 0.3963\n",
      "Epoch 6/20\n",
      "38/38 [==============================] - 2s 50ms/step - loss: 1.4463 - accuracy: 0.4130\n",
      "Epoch 7/20\n",
      "38/38 [==============================] - 2s 50ms/step - loss: 1.4182 - accuracy: 0.4430\n",
      "Epoch 8/20\n",
      "38/38 [==============================] - 2s 53ms/step - loss: 1.3824 - accuracy: 0.4596\n",
      "Epoch 9/20\n",
      "38/38 [==============================] - 2s 52ms/step - loss: 1.3612 - accuracy: 0.4871\n",
      "Epoch 10/20\n",
      "38/38 [==============================] - 2s 51ms/step - loss: 1.3265 - accuracy: 0.4896\n",
      "Epoch 11/20\n",
      "38/38 [==============================] - 2s 52ms/step - loss: 1.3180 - accuracy: 0.4829\n",
      "Epoch 12/20\n",
      "38/38 [==============================] - 2s 50ms/step - loss: 1.2929 - accuracy: 0.4921\n",
      "Epoch 13/20\n",
      "38/38 [==============================] - 2s 51ms/step - loss: 1.2792 - accuracy: 0.5096\n",
      "Epoch 14/20\n",
      "38/38 [==============================] - 2s 55ms/step - loss: 1.2602 - accuracy: 0.5196\n",
      "Epoch 15/20\n",
      "38/38 [==============================] - 2s 53ms/step - loss: 1.2591 - accuracy: 0.5137\n",
      "Epoch 16/20\n",
      "38/38 [==============================] - 2s 52ms/step - loss: 1.2392 - accuracy: 0.5246\n",
      "Epoch 17/20\n",
      "38/38 [==============================] - 2s 52ms/step - loss: 1.2219 - accuracy: 0.5420\n",
      "Epoch 18/20\n",
      "38/38 [==============================] - 2s 57ms/step - loss: 1.2090 - accuracy: 0.5587\n",
      "Epoch 19/20\n",
      "38/38 [==============================] - 2s 57ms/step - loss: 1.2012 - accuracy: 0.5495\n",
      "Epoch 20/20\n",
      "38/38 [==============================] - 2s 55ms/step - loss: 1.1954 - accuracy: 0.5537\n"
     ]
    }
   ],
   "source": [
    "history = classifier_model.fit(x=np.array(dm.train_str_questions), y=dm.train_numeral_labels, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys       : ['input_type_ids', 'input_word_ids', 'input_mask']\n",
      "Shape      : (299, 128)\n",
      "Word Ids   : [ 101 2103 2054 2103 1005 1055 3780 2003 2170 1036 1036 1996]\n",
      "Input Mask : [1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Type Ids   : [0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "#Insert your code for extracting training set\n",
    "pre_x_valid = preprocessor(dm.valid_str_questions)\n",
    "print(f'Keys       : {list(pre_x_valid.keys())}')\n",
    "print(f'Shape      : {pre_x_valid[\"input_word_ids\"].shape}')\n",
    "print(f'Word Ids   : {pre_x_valid[\"input_word_ids\"][0, :12]}')\n",
    "print(f'Input Mask : {pre_x_valid[\"input_mask\"][0, :12]}')\n",
    "print(f'Type Ids   : {pre_x_valid[\"input_type_ids\"][0, :12]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys       : ['input_type_ids', 'input_word_ids', 'input_mask']\n",
      "Shape      : (1201, 128)\n",
      "Word Ids   : [  101  5450  2129  2106 14262  2546  9527  4503  1999  1998  2059  2681]\n",
      "Input Mask : [1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Type Ids   : [0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "#Insert your code for extracting training set\n",
    "pre_x_train = preprocessor(dm.train_str_questions)\n",
    "print(f'Keys       : {list(pre_x_train.keys())}')\n",
    "print(f'Shape      : {pre_x_train[\"input_word_ids\"].shape}')\n",
    "print(f'Word Ids   : {pre_x_train[\"input_word_ids\"][0, :12]}')\n",
    "print(f'Input Mask : {pre_x_train[\"input_mask\"][0, :12]}')\n",
    "print(f'Type Ids   : {pre_x_train[\"input_type_ids\"][0, :12]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">**Question 4.2**</span>\n",
    "**Use Logistic Regression to train on the training set and then evaluate on the valid set.**\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[4 points]</span></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\US\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression \n",
    "\n",
    "clf = LogisticRegression(random_state=0, solver='liblinear').fit(pre_x_train['input_word_ids'], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy on the valid dataset is 0.6387959866220736\n"
     ]
    }
   ],
   "source": [
    "#Insert your code for testing here\n",
    "acc = clf.score(pre_x_valid['input_word_ids'], y_valid)\n",
    "print(f\"The accuracy on the valid dataset is {acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#0b486b\">Part 5: RNNs for sequence modeling and neural embedding </span>\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red; font-weight:bold\">[Total marks for this part: 55 points]<span></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#0b486b\">5.1. One-directional RNNs for sequence modeling and neural embedding </span> ###\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red; font-weight:bold\">[Total marks for this part: 10 points]<span></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">**Question 5.1.1**</span> \n",
    "**In this part, you need to construct an RNN to learn from the dataset of interest. Basically, you are required first to construct the class UniRNN (Uni-directional RNN) with the following requirements:**\n",
    "- Attribute `data_manager (self.data_manager)`: specifies the data manager used to store data for the model.\n",
    "- Attribute `cell_type (self.cell_type)`: can receive three values including `basic_rnn`, `gru`, and `lstm` which specifies the memory cells formed a hidden layer.\n",
    "- `state_sizes (self.state_sizes)` indicates the list of the hidden sizes  of memory cells. For example, $state\\_sizes = [64, 128]$ means that you have two hidden layers in your network with hidden sizes of $64$ and $128$ respectively.\n",
    "\n",
    "**Note that when declaring an embedding layer for the network, you need to set *mask_zero=True* so that the padding zeros in the sentences will be masked and ignored. This helps to have variable length RNNs. For more detail, you can refer to this [link](https://www.tensorflow.org/guide/keras/masking_and_padding).**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[7 points]</span></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UniRNN:\n",
    "    def __init__(self, cell_type='gru', embed_size=128, state_sizes=[128, 64], data_manager=None):\n",
    "        self.cell_type = cell_type\n",
    "        self.state_sizes = state_sizes\n",
    "        self.embed_size = embed_size\n",
    "        self.data_manager = data_manager\n",
    "        self.vocab_size = self.data_manager.vocab_size + 1\n",
    "        self.embed_matrix = np.zeros((self.vocab_size, self.embed_size))\n",
    "\n",
    "    # return the correspoding memory cell\n",
    "    @staticmethod\n",
    "    def get_layer(cell_type='gru', state_size=128, return_sequences=True, activation='tanh'):\n",
    "        if cell_type == 'gru':\n",
    "            return tf.keras.layers.GRU(state_size, return_sequences=return_sequences, activation=activation)\n",
    "        elif cell_type == 'lstm':\n",
    "            return tf.keras.layers.LSTM(state_size, return_sequences=return_sequences, activation=activation)\n",
    "        else:\n",
    "            return tf.keras.layers.RNN(\n",
    "                tf.keras.layers.SimpleRNNCell(state_size, activation=activation),\n",
    "                return_sequences=return_sequences)\n",
    "\n",
    "    def build(self):\n",
    "        x = tf.keras.layers.Input(shape=[None])\n",
    "        h = tf.keras.layers.Embedding(\n",
    "            self.vocab_size, self.embed_size, mask_zero=True)(x)\n",
    "        num_layers = len(self.state_sizes)\n",
    "        for i in range(num_layers):\n",
    "            if i == num_layers - 1:\n",
    "                h = self.get_layer(cell_type=self.cell_type,\n",
    "                               state_size=self.state_sizes[i], return_sequences=False)(h)\n",
    "            else:\n",
    "                h = self.get_layer(cell_type=self.cell_type,\n",
    "                                state_size=self.state_sizes[i])(h)\n",
    "        h = tf.keras.layers.Dense(self.data_manager.num_classes, activation='softmax')(h)\n",
    "        self.model = tf.keras.Model(inputs=x, outputs=h)\n",
    "\n",
    "    def compile_model(self, *args, **kwargs):\n",
    "        self.model.compile(*args, **kwargs)\n",
    "\n",
    "    def fit(self, *args, **kwargs):\n",
    "        return self.model.fit(*args, **kwargs)\n",
    "\n",
    "    def evaluate(self, *args, **kwargs):\n",
    "        self.model.evaluate(*args, **kwargs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">**Question 5.1.2**</span> \n",
    "**Run with basic RNN ('basic_rnn') cell with $embed\\_size= 128, state\\_sizes= [64, 128], data\\_manager= dm$.**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[1 points]</span></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "19/19 [==============================] - 2s 51ms/step - loss: 1.1063 - accuracy: 0.5737 - val_loss: 0.6174 - val_accuracy: 0.8595\n",
      "Epoch 2/20\n",
      "19/19 [==============================] - 1s 40ms/step - loss: 0.3714 - accuracy: 0.8793 - val_loss: 0.3352 - val_accuracy: 0.9197\n",
      "Epoch 3/20\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 0.1318 - accuracy: 0.9675 - val_loss: 0.3151 - val_accuracy: 0.9298\n",
      "Epoch 4/20\n",
      "19/19 [==============================] - 1s 42ms/step - loss: 0.0568 - accuracy: 0.9850 - val_loss: 0.2494 - val_accuracy: 0.9465\n",
      "Epoch 5/20\n",
      "19/19 [==============================] - 1s 42ms/step - loss: 0.0353 - accuracy: 0.9917 - val_loss: 0.4082 - val_accuracy: 0.9365\n",
      "Epoch 6/20\n",
      "19/19 [==============================] - 1s 42ms/step - loss: 0.0484 - accuracy: 0.9833 - val_loss: 0.1527 - val_accuracy: 0.9498\n",
      "Epoch 7/20\n",
      "19/19 [==============================] - 1s 42ms/step - loss: 0.0226 - accuracy: 0.9950 - val_loss: 0.0936 - val_accuracy: 0.9632\n",
      "Epoch 8/20\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 0.0049 - accuracy: 1.0000 - val_loss: 0.0951 - val_accuracy: 0.9732\n",
      "Epoch 9/20\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.0951 - val_accuracy: 0.9666\n",
      "Epoch 10/20\n",
      "19/19 [==============================] - 1s 39ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.1370 - val_accuracy: 0.9666\n",
      "Epoch 11/20\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 0.0918 - accuracy: 0.9734 - val_loss: 0.1293 - val_accuracy: 0.9632\n",
      "Epoch 12/20\n",
      "19/19 [==============================] - 1s 40ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.1275 - val_accuracy: 0.9666\n",
      "Epoch 13/20\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 6.8612e-04 - accuracy: 1.0000 - val_loss: 0.1286 - val_accuracy: 0.9666\n",
      "Epoch 14/20\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 4.4955e-04 - accuracy: 1.0000 - val_loss: 0.1339 - val_accuracy: 0.9632\n",
      "Epoch 15/20\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 2.8159e-04 - accuracy: 1.0000 - val_loss: 0.1425 - val_accuracy: 0.9565\n",
      "Epoch 16/20\n",
      "19/19 [==============================] - 1s 40ms/step - loss: 1.5934e-04 - accuracy: 1.0000 - val_loss: 0.1559 - val_accuracy: 0.9565\n",
      "Epoch 17/20\n",
      "19/19 [==============================] - 1s 42ms/step - loss: 8.3290e-05 - accuracy: 1.0000 - val_loss: 0.1701 - val_accuracy: 0.9565\n",
      "Epoch 18/20\n",
      "19/19 [==============================] - 1s 40ms/step - loss: 3.7787e-05 - accuracy: 1.0000 - val_loss: 0.1729 - val_accuracy: 0.9532\n",
      "Epoch 19/20\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 1.8110e-05 - accuracy: 1.0000 - val_loss: 0.1882 - val_accuracy: 0.9532\n",
      "Epoch 20/20\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 7.1079e-06 - accuracy: 1.0000 - val_loss: 0.1927 - val_accuracy: 0.9565\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2874f7b5880>"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uni_rnn = UniRNN('basic_rnn', embed_size=128, state_sizes=[64,128], data_manager=dm)\n",
    "uni_rnn.build()\n",
    "opt = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "uni_rnn.compile_model(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "uni_rnn.fit(dm.tf_train_set.batch(64), epochs=20, validation_data = dm.tf_valid_set.batch(64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">**Question 5.1.3**</span> \n",
    "**Run with GRU ('gru') cell with $embed\\_size= 128, state\\_sizes= [64, 128], data\\_manager= dm$.**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[1 points]</span></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "19/19 [==============================] - 7s 189ms/step - loss: 1.5639 - accuracy: 0.3514 - val_loss: 0.9807 - val_accuracy: 0.6823\n",
      "Epoch 2/20\n",
      "19/19 [==============================] - 2s 115ms/step - loss: 0.7324 - accuracy: 0.7186 - val_loss: 0.4318 - val_accuracy: 0.9030\n",
      "Epoch 3/20\n",
      "19/19 [==============================] - 2s 117ms/step - loss: 0.2791 - accuracy: 0.9284 - val_loss: 0.2193 - val_accuracy: 0.9398\n",
      "Epoch 4/20\n",
      "19/19 [==============================] - 2s 115ms/step - loss: 0.1065 - accuracy: 0.9750 - val_loss: 0.1828 - val_accuracy: 0.9431\n",
      "Epoch 5/20\n",
      "19/19 [==============================] - 2s 115ms/step - loss: 0.0842 - accuracy: 0.9784 - val_loss: 0.1931 - val_accuracy: 0.9398\n",
      "Epoch 6/20\n",
      "19/19 [==============================] - 2s 110ms/step - loss: 0.0528 - accuracy: 0.9867 - val_loss: 0.1322 - val_accuracy: 0.9599\n",
      "Epoch 7/20\n",
      "19/19 [==============================] - 2s 118ms/step - loss: 0.0186 - accuracy: 0.9967 - val_loss: 0.1436 - val_accuracy: 0.9599\n",
      "Epoch 8/20\n",
      "19/19 [==============================] - 2s 111ms/step - loss: 0.0431 - accuracy: 0.9833 - val_loss: 0.1382 - val_accuracy: 0.9565\n",
      "Epoch 9/20\n",
      "19/19 [==============================] - 2s 111ms/step - loss: 0.0054 - accuracy: 0.9992 - val_loss: 0.1465 - val_accuracy: 0.9565\n",
      "Epoch 10/20\n",
      "19/19 [==============================] - 2s 108ms/step - loss: 0.0069 - accuracy: 0.9983 - val_loss: 0.1706 - val_accuracy: 0.9532\n",
      "Epoch 11/20\n",
      "19/19 [==============================] - 2s 110ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.1890 - val_accuracy: 0.9532\n",
      "Epoch 12/20\n",
      "19/19 [==============================] - 2s 107ms/step - loss: 7.5805e-04 - accuracy: 1.0000 - val_loss: 0.2199 - val_accuracy: 0.9532\n",
      "Epoch 13/20\n",
      "19/19 [==============================] - 2s 111ms/step - loss: 3.3972e-04 - accuracy: 1.0000 - val_loss: 0.2512 - val_accuracy: 0.9532\n",
      "Epoch 14/20\n",
      "19/19 [==============================] - 2s 107ms/step - loss: 1.4580e-04 - accuracy: 1.0000 - val_loss: 0.2887 - val_accuracy: 0.9532\n",
      "Epoch 15/20\n",
      "19/19 [==============================] - 2s 112ms/step - loss: 6.2242e-05 - accuracy: 1.0000 - val_loss: 0.3214 - val_accuracy: 0.9532\n",
      "Epoch 16/20\n",
      "19/19 [==============================] - 2s 109ms/step - loss: 2.7476e-05 - accuracy: 1.0000 - val_loss: 0.3513 - val_accuracy: 0.9532\n",
      "Epoch 17/20\n",
      "19/19 [==============================] - 2s 115ms/step - loss: 1.2420e-05 - accuracy: 1.0000 - val_loss: 0.3815 - val_accuracy: 0.9532\n",
      "Epoch 18/20\n",
      "19/19 [==============================] - 2s 110ms/step - loss: 5.8040e-06 - accuracy: 1.0000 - val_loss: 0.4125 - val_accuracy: 0.9532\n",
      "Epoch 19/20\n",
      "19/19 [==============================] - 2s 114ms/step - loss: 2.8401e-06 - accuracy: 1.0000 - val_loss: 0.4409 - val_accuracy: 0.9498\n",
      "Epoch 20/20\n",
      "19/19 [==============================] - 2s 110ms/step - loss: 1.4819e-06 - accuracy: 1.0000 - val_loss: 0.4641 - val_accuracy: 0.9498\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x28757f10610>"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uni_rnn = UniRNN('gru', embed_size=128, state_sizes=[64,128], data_manager=dm)\n",
    "uni_rnn.build()\n",
    "opt = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "uni_rnn.compile_model(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "uni_rnn.fit(dm.tf_train_set.batch(64), epochs=20, validation_data = dm.tf_valid_set.batch(64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">**Question 5.1.4**</span> \n",
    "**Run with LSTM ('lstm') cell with $embed\\_size= 128, state\\_sizes= [64, 128], data\\_manager= dm$.**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[1 points]</span></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "19/19 [==============================] - 8s 221ms/step - loss: 1.5089 - accuracy: 0.4480 - val_loss: 1.0912 - val_accuracy: 0.7592\n",
      "Epoch 2/20\n",
      "19/19 [==============================] - 2s 128ms/step - loss: 0.5596 - accuracy: 0.8293 - val_loss: 0.5256 - val_accuracy: 0.8963\n",
      "Epoch 3/20\n",
      "19/19 [==============================] - 2s 127ms/step - loss: 0.2152 - accuracy: 0.9467 - val_loss: 0.2901 - val_accuracy: 0.9130\n",
      "Epoch 4/20\n",
      "19/19 [==============================] - 2s 129ms/step - loss: 0.1149 - accuracy: 0.9734 - val_loss: 0.1596 - val_accuracy: 0.9498\n",
      "Epoch 5/20\n",
      "19/19 [==============================] - 2s 128ms/step - loss: 0.0624 - accuracy: 0.9867 - val_loss: 0.1536 - val_accuracy: 0.9498\n",
      "Epoch 6/20\n",
      "19/19 [==============================] - 2s 129ms/step - loss: 0.0393 - accuracy: 0.9900 - val_loss: 0.1842 - val_accuracy: 0.9465\n",
      "Epoch 7/20\n",
      "19/19 [==============================] - 2s 128ms/step - loss: 0.0158 - accuracy: 0.9967 - val_loss: 0.2138 - val_accuracy: 0.9465\n",
      "Epoch 8/20\n",
      "19/19 [==============================] - 2s 127ms/step - loss: 0.0098 - accuracy: 0.9992 - val_loss: 0.1249 - val_accuracy: 0.9666\n",
      "Epoch 9/20\n",
      "19/19 [==============================] - 2s 127ms/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.1431 - val_accuracy: 0.9599\n",
      "Epoch 10/20\n",
      "19/19 [==============================] - 2s 128ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.1647 - val_accuracy: 0.9565\n",
      "Epoch 11/20\n",
      "19/19 [==============================] - 2s 128ms/step - loss: 0.0074 - accuracy: 0.9983 - val_loss: 0.1524 - val_accuracy: 0.9732\n",
      "Epoch 12/20\n",
      "19/19 [==============================] - 2s 129ms/step - loss: 3.8259e-04 - accuracy: 1.0000 - val_loss: 0.1577 - val_accuracy: 0.9732\n",
      "Epoch 13/20\n",
      "19/19 [==============================] - 2s 127ms/step - loss: 2.2802e-04 - accuracy: 1.0000 - val_loss: 0.1616 - val_accuracy: 0.9732\n",
      "Epoch 14/20\n",
      "19/19 [==============================] - 2s 128ms/step - loss: 1.3308e-04 - accuracy: 1.0000 - val_loss: 0.1730 - val_accuracy: 0.9732\n",
      "Epoch 15/20\n",
      "19/19 [==============================] - 2s 132ms/step - loss: 7.0163e-05 - accuracy: 1.0000 - val_loss: 0.1826 - val_accuracy: 0.9732\n",
      "Epoch 16/20\n",
      "19/19 [==============================] - 2s 129ms/step - loss: 3.5283e-05 - accuracy: 1.0000 - val_loss: 0.1951 - val_accuracy: 0.9732\n",
      "Epoch 17/20\n",
      "19/19 [==============================] - 2s 127ms/step - loss: 1.7287e-05 - accuracy: 1.0000 - val_loss: 0.2076 - val_accuracy: 0.9732\n",
      "Epoch 18/20\n",
      "19/19 [==============================] - 2s 128ms/step - loss: 8.4948e-06 - accuracy: 1.0000 - val_loss: 0.2317 - val_accuracy: 0.9699\n",
      "Epoch 19/20\n",
      "19/19 [==============================] - 2s 128ms/step - loss: 4.2730e-06 - accuracy: 1.0000 - val_loss: 0.2610 - val_accuracy: 0.9666\n",
      "Epoch 20/20\n",
      "19/19 [==============================] - 2s 130ms/step - loss: 2.2276e-06 - accuracy: 1.0000 - val_loss: 0.2951 - val_accuracy: 0.9666\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2875bae61f0>"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uni_rnn = UniRNN('lstm', embed_size=128, state_sizes=[64,128], data_manager=dm)\n",
    "uni_rnn.build()\n",
    "opt = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "uni_rnn.compile_model(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "uni_rnn.fit(dm.tf_train_set.batch(64), epochs=20, validation_data = dm.tf_valid_set.batch(64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#0b486b\">5.2. Bi-directional RNNs for sequence modeling and neural embedding </span> ###\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red; font-weight:bold\">[Total marks for this part: 10 points]<span></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">**Question 5.2.1**</span> \n",
    "**In what follow, you will investigate BiRNN. The task is similar to Part 5.1 but you need to write the code for an BiRNN. Note that the function *get_layer(cell_type= 'gru', state_size= 128, return_sequences= False, activation = 'tanh')* has to return the hidden layer with bidirectional memory cells (e.g., Basic RNN, GRU, and LSTM cells).**\n",
    "\n",
    "**Complete the code of the class *BiRNN*. Note that for the embedding layer you need to set *mask_zero=True*.**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[7 points]</span></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiRNN:\n",
    "    def __init__(self, cell_type= 'gru', embed_size= 128, state_sizes= [128, 64], data_manager= None):\n",
    "        self.cell_type = cell_type\n",
    "        self.state_sizes = state_sizes\n",
    "        self.embed_size = embed_size\n",
    "        self.data_manager = data_manager\n",
    "        self.vocab_size = self.data_manager.vocab_size +1\n",
    "        \n",
    "    # return the correspoding memory cell\n",
    "    @staticmethod\n",
    "    def get_layer(cell_type='gru', state_size=128, return_sequences=True, activation='tanh'):\n",
    "        if cell_type == 'gru':\n",
    "            return tf.keras.layers.Bidirectional(tf.keras.layers.GRU(state_size, return_sequences=return_sequences, activation=activation))\n",
    "        elif cell_type == 'lstm':\n",
    "            return tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(state_size, return_sequences=return_sequences, activation=activation))\n",
    "        else:\n",
    "            return tf.keras.layers.Bidirectional(tf.keras.layers.RNN(\n",
    "                tf.keras.layers.SimpleRNNCell(state_size, activation=activation),\n",
    "                return_sequences=return_sequences))\n",
    "    \n",
    "    def build(self):\n",
    "        x = tf.keras.layers.Input(shape=[None])\n",
    "        h = tf.keras.layers.Embedding(\n",
    "            self.vocab_size, self.embed_size, mask_zero=True)(x)\n",
    "        num_layers = len(self.state_sizes)\n",
    "        for i in range(num_layers):\n",
    "            if i == num_layers - 1:\n",
    "                h = self.get_layer(cell_type=self.cell_type,\n",
    "                               state_size=self.state_sizes[i], return_sequences=False)(h)\n",
    "            else:\n",
    "                h = self.get_layer(cell_type=self.cell_type,\n",
    "                                state_size=self.state_sizes[i])(h)\n",
    "        h = tf.keras.layers.Dense(dm.num_classes, activation='softmax')(h)\n",
    "        self.model = tf.keras.Model(inputs=x, outputs=h)\n",
    "        \n",
    "    \n",
    "    def compile_model(self, *args, **kwargs):\n",
    "        self.model.compile(*args, **kwargs)\n",
    "    \n",
    "    def fit(self, *args, **kwargs):\n",
    "        return self.model.fit(*args, **kwargs)\n",
    "    \n",
    "    def evaluate(self, *args, **kwargs):\n",
    "        self.model.evaluate(*args, **kwargs)       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">**Question 5.2.2**</span> \n",
    "**Run BiRNN for basic RNN ('basic_rnn') cell with $embed\\_size= 128, state\\_sizes= [64, 128], data\\_manager= dm$.**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[1 points]</span></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "19/19 [==============================] - 4s 101ms/step - loss: 0.7146 - accuracy: 0.7752 - val_loss: 0.2792 - val_accuracy: 0.9298\n",
      "Epoch 2/20\n",
      "19/19 [==============================] - 2s 83ms/step - loss: 0.0869 - accuracy: 0.9767 - val_loss: 0.3460 - val_accuracy: 0.9264\n",
      "Epoch 3/20\n",
      "19/19 [==============================] - 2s 87ms/step - loss: 0.0268 - accuracy: 0.9950 - val_loss: 0.1169 - val_accuracy: 0.9532\n",
      "Epoch 4/20\n",
      "19/19 [==============================] - 2s 88ms/step - loss: 0.0039 - accuracy: 0.9992 - val_loss: 0.1137 - val_accuracy: 0.9565\n",
      "Epoch 5/20\n",
      "19/19 [==============================] - 2s 89ms/step - loss: 9.5775e-04 - accuracy: 1.0000 - val_loss: 0.1307 - val_accuracy: 0.9532\n",
      "Epoch 6/20\n",
      "19/19 [==============================] - 2s 89ms/step - loss: 2.9662e-04 - accuracy: 1.0000 - val_loss: 0.1515 - val_accuracy: 0.9565\n",
      "Epoch 7/20\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 1.0109e-04 - accuracy: 1.0000 - val_loss: 0.1769 - val_accuracy: 0.9565\n",
      "Epoch 8/20\n",
      "19/19 [==============================] - 2s 89ms/step - loss: 3.5857e-05 - accuracy: 1.0000 - val_loss: 0.2044 - val_accuracy: 0.9532\n",
      "Epoch 9/20\n",
      "19/19 [==============================] - 2s 89ms/step - loss: 1.2989e-05 - accuracy: 1.0000 - val_loss: 0.2319 - val_accuracy: 0.9532\n",
      "Epoch 10/20\n",
      "19/19 [==============================] - 2s 93ms/step - loss: 4.7759e-06 - accuracy: 1.0000 - val_loss: 0.2594 - val_accuracy: 0.9532\n",
      "Epoch 11/20\n",
      "19/19 [==============================] - 2s 91ms/step - loss: 1.8335e-06 - accuracy: 1.0000 - val_loss: 0.2829 - val_accuracy: 0.9498\n",
      "Epoch 12/20\n",
      "19/19 [==============================] - 2s 93ms/step - loss: 7.4652e-07 - accuracy: 1.0000 - val_loss: 0.3066 - val_accuracy: 0.9498\n",
      "Epoch 13/20\n",
      "19/19 [==============================] - 2s 89ms/step - loss: 0.0078 - accuracy: 0.9975 - val_loss: 0.3596 - val_accuracy: 0.9431\n",
      "Epoch 14/20\n",
      "19/19 [==============================] - 2s 91ms/step - loss: 0.0066 - accuracy: 0.9958 - val_loss: 0.2605 - val_accuracy: 0.9565\n",
      "Epoch 15/20\n",
      "19/19 [==============================] - 2s 91ms/step - loss: 3.9080e-05 - accuracy: 1.0000 - val_loss: 0.2382 - val_accuracy: 0.9565\n",
      "Epoch 16/20\n",
      "19/19 [==============================] - 2s 89ms/step - loss: 6.2852e-06 - accuracy: 1.0000 - val_loss: 0.2422 - val_accuracy: 0.9532\n",
      "Epoch 17/20\n",
      "19/19 [==============================] - 2s 91ms/step - loss: 4.1147e-06 - accuracy: 1.0000 - val_loss: 0.2483 - val_accuracy: 0.9532\n",
      "Epoch 18/20\n",
      "19/19 [==============================] - 2s 88ms/step - loss: 2.6603e-06 - accuracy: 1.0000 - val_loss: 0.2537 - val_accuracy: 0.9565\n",
      "Epoch 19/20\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 1.6976e-06 - accuracy: 1.0000 - val_loss: 0.2582 - val_accuracy: 0.9565\n",
      "Epoch 20/20\n",
      "19/19 [==============================] - 2s 88ms/step - loss: 9.7203e-07 - accuracy: 1.0000 - val_loss: 0.2680 - val_accuracy: 0.9532\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x28757e659d0>"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_rnn = BiRNN('basic_rnn', embed_size=128, state_sizes=[64,128], data_manager=dm)\n",
    "bi_rnn.build()\n",
    "opt = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "bi_rnn.compile_model(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "bi_rnn.fit(dm.tf_train_set.batch(64), epochs=20, validation_data = dm.tf_valid_set.batch(64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">**Question 5.2.3**</span> \n",
    "**Run BiRNN for GRU ('gru') cell with $embed\\_size= 128, state\\_sizes= [64, 128], data\\_manager= dm$.**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[1 points]</span></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "19/19 [==============================] - 18s 571ms/step - loss: 1.3020 - accuracy: 0.5520 - val_loss: 0.7452 - val_accuracy: 0.7525\n",
      "Epoch 2/20\n",
      "19/19 [==============================] - 9s 489ms/step - loss: 0.3549 - accuracy: 0.8993 - val_loss: 0.2939 - val_accuracy: 0.8997\n",
      "Epoch 3/20\n",
      "19/19 [==============================] - 8s 430ms/step - loss: 0.1053 - accuracy: 0.9692 - val_loss: 0.2425 - val_accuracy: 0.9264\n",
      "Epoch 4/20\n",
      "19/19 [==============================] - 9s 484ms/step - loss: 0.0329 - accuracy: 0.9883 - val_loss: 0.1640 - val_accuracy: 0.9431\n",
      "Epoch 5/20\n",
      "19/19 [==============================] - 8s 434ms/step - loss: 0.0079 - accuracy: 0.9983 - val_loss: 0.1810 - val_accuracy: 0.9498\n",
      "Epoch 6/20\n",
      "19/19 [==============================] - 9s 495ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.2053 - val_accuracy: 0.9532\n",
      "Epoch 7/20\n",
      "19/19 [==============================] - 8s 416ms/step - loss: 4.4185e-04 - accuracy: 1.0000 - val_loss: 0.2321 - val_accuracy: 0.9498\n",
      "Epoch 8/20\n",
      "19/19 [==============================] - 10s 507ms/step - loss: 1.4531e-04 - accuracy: 1.0000 - val_loss: 0.2551 - val_accuracy: 0.9532\n",
      "Epoch 9/20\n",
      "19/19 [==============================] - 8s 421ms/step - loss: 4.9551e-05 - accuracy: 1.0000 - val_loss: 0.2806 - val_accuracy: 0.9532\n",
      "Epoch 10/20\n",
      "19/19 [==============================] - 11s 578ms/step - loss: 1.8065e-05 - accuracy: 1.0000 - val_loss: 0.3142 - val_accuracy: 0.9498\n",
      "Epoch 11/20\n",
      "19/19 [==============================] - 8s 423ms/step - loss: 6.9960e-06 - accuracy: 1.0000 - val_loss: 0.3486 - val_accuracy: 0.9498\n",
      "Epoch 12/20\n",
      "19/19 [==============================] - 10s 542ms/step - loss: 2.8721e-06 - accuracy: 1.0000 - val_loss: 0.3764 - val_accuracy: 0.9498\n",
      "Epoch 13/20\n",
      "19/19 [==============================] - 8s 422ms/step - loss: 1.2688e-06 - accuracy: 1.0000 - val_loss: 0.4013 - val_accuracy: 0.9498\n",
      "Epoch 14/20\n",
      "19/19 [==============================] - 10s 502ms/step - loss: 6.0417e-07 - accuracy: 1.0000 - val_loss: 0.4202 - val_accuracy: 0.9465\n",
      "Epoch 15/20\n",
      "19/19 [==============================] - 8s 413ms/step - loss: 3.1395e-07 - accuracy: 1.0000 - val_loss: 0.4355 - val_accuracy: 0.9498\n",
      "Epoch 16/20\n",
      "19/19 [==============================] - 10s 506ms/step - loss: 1.7430e-07 - accuracy: 1.0000 - val_loss: 0.4459 - val_accuracy: 0.9465\n",
      "Epoch 17/20\n",
      "19/19 [==============================] - 8s 433ms/step - loss: 1.0670e-07 - accuracy: 1.0000 - val_loss: 0.4191 - val_accuracy: 0.9565\n",
      "Epoch 18/20\n",
      "19/19 [==============================] - 10s 506ms/step - loss: 7.2955e-08 - accuracy: 1.0000 - val_loss: 0.4249 - val_accuracy: 0.9565\n",
      "Epoch 19/20\n",
      "19/19 [==============================] - 8s 432ms/step - loss: 5.4096e-08 - accuracy: 1.0000 - val_loss: 0.4283 - val_accuracy: 0.9565\n",
      "Epoch 20/20\n",
      "19/19 [==============================] - 9s 479ms/step - loss: 4.0597e-08 - accuracy: 1.0000 - val_loss: 0.4317 - val_accuracy: 0.9565\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x287895fcaf0>"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_rnn = BiRNN('gru', embed_size=128, state_sizes=[64,128], data_manager=dm)\n",
    "bi_rnn.build()\n",
    "opt = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "bi_rnn.compile_model(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "bi_rnn.fit(dm.tf_train_set.batch(64), epochs=20, validation_data = dm.tf_valid_set.batch(64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">**Question 5.2.4**</span> \n",
    "**Run BiRNN for LSTM ('lstm') cell with $embed\\_size= 128, state\\_sizes= [64, 128], data\\_manager= dm$.**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[1 points]</span></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "19/19 [==============================] - 21s 739ms/step - loss: 1.3705 - accuracy: 0.5121 - val_loss: 0.8787 - val_accuracy: 0.7926\n",
      "Epoch 2/20\n",
      "19/19 [==============================] - 12s 653ms/step - loss: 0.4227 - accuracy: 0.8934 - val_loss: 0.5163 - val_accuracy: 0.8796\n",
      "Epoch 3/20\n",
      "19/19 [==============================] - 11s 595ms/step - loss: 0.1801 - accuracy: 0.9534 - val_loss: 0.2543 - val_accuracy: 0.9331\n",
      "Epoch 4/20\n",
      "19/19 [==============================] - 13s 660ms/step - loss: 0.0636 - accuracy: 0.9858 - val_loss: 0.1980 - val_accuracy: 0.9565\n",
      "Epoch 5/20\n",
      "19/19 [==============================] - 11s 591ms/step - loss: 0.0212 - accuracy: 0.9967 - val_loss: 0.1993 - val_accuracy: 0.9498\n",
      "Epoch 6/20\n",
      "19/19 [==============================] - 13s 658ms/step - loss: 0.0121 - accuracy: 0.9975 - val_loss: 0.8230 - val_accuracy: 0.8562\n",
      "Epoch 7/20\n",
      "19/19 [==============================] - 11s 596ms/step - loss: 0.0461 - accuracy: 0.9892 - val_loss: 0.1667 - val_accuracy: 0.9532\n",
      "Epoch 8/20\n",
      "19/19 [==============================] - 13s 666ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.1824 - val_accuracy: 0.9532\n",
      "Epoch 9/20\n",
      "19/19 [==============================] - 11s 593ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.2018 - val_accuracy: 0.9498\n",
      "Epoch 10/20\n",
      "19/19 [==============================] - 12s 652ms/step - loss: 5.5318e-04 - accuracy: 1.0000 - val_loss: 0.2270 - val_accuracy: 0.9498\n",
      "Epoch 11/20\n",
      "19/19 [==============================] - 11s 589ms/step - loss: 2.6142e-04 - accuracy: 1.0000 - val_loss: 0.2542 - val_accuracy: 0.9532\n",
      "Epoch 12/20\n",
      "19/19 [==============================] - 12s 641ms/step - loss: 0.0205 - accuracy: 0.9942 - val_loss: 0.2970 - val_accuracy: 0.9431\n",
      "Epoch 13/20\n",
      "19/19 [==============================] - 11s 591ms/step - loss: 1.6930e-04 - accuracy: 1.0000 - val_loss: 0.2919 - val_accuracy: 0.9498\n",
      "Epoch 14/20\n",
      "19/19 [==============================] - 12s 644ms/step - loss: 9.0866e-05 - accuracy: 1.0000 - val_loss: 0.2928 - val_accuracy: 0.9532\n",
      "Epoch 15/20\n",
      "19/19 [==============================] - 11s 578ms/step - loss: 6.3124e-05 - accuracy: 1.0000 - val_loss: 0.2928 - val_accuracy: 0.9565\n",
      "Epoch 16/20\n",
      "19/19 [==============================] - 12s 648ms/step - loss: 4.0382e-05 - accuracy: 1.0000 - val_loss: 0.2933 - val_accuracy: 0.9599\n",
      "Epoch 17/20\n",
      "19/19 [==============================] - 11s 572ms/step - loss: 2.3699e-05 - accuracy: 1.0000 - val_loss: 0.2972 - val_accuracy: 0.9532\n",
      "Epoch 18/20\n",
      "19/19 [==============================] - 12s 636ms/step - loss: 1.2910e-05 - accuracy: 1.0000 - val_loss: 0.3081 - val_accuracy: 0.9532\n",
      "Epoch 19/20\n",
      "19/19 [==============================] - 11s 575ms/step - loss: 6.6527e-06 - accuracy: 1.0000 - val_loss: 0.3114 - val_accuracy: 0.9632\n",
      "Epoch 20/20\n",
      "19/19 [==============================] - 12s 636ms/step - loss: 3.3462e-06 - accuracy: 1.0000 - val_loss: 0.3088 - val_accuracy: 0.9632\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x287a0ae66a0>"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_rnn = bi_rnn = BiRNN('lstm', embed_size=128, state_sizes=[64,128], data_manager=dm)\n",
    "bi_rnn.build()\n",
    "opt = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "bi_rnn.compile_model(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "bi_rnn.fit(dm.tf_train_set.batch(64), epochs=20, validation_data = dm.tf_valid_set.batch(64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#0b486b\">5.3. RNNs with various types, cells, and fine-tuning embedding matrix for sequence modeling and neural embedding </span> ###\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red; font-weight:bold\">[Total marks for this part: 11 points]<span></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">**Question 5.3.1**</span> \n",
    "\n",
    "**In what follows, you are required to combine the code in Part 1 and Part 2 to gain a general RNN which can be either Uni-directional RNN or Bi-directional RNN and the embedding matrix can be initialized using a pretrained Word2Vect.**\n",
    "\n",
    "**Below are the descriptions of the attributes of the class *RNN*:**\n",
    "- `run_mode (self.run_mode)` has three values (scratch, init-only, and init-fine-tune).\n",
    "  - `scratch` means training the embedding matrix from scratch.\n",
    "  - `init-only` means only initializing the embedding matrix with a pretrained Word2Vect but **not further doing** fine-tuning that matrix.\n",
    "  - `init-fine-tune` means both initializing the embedding matrix with a pretrained Word2Vect and **further doing** fine-tuning that matrix.\n",
    "- `network_type (self.network_type)` has two values (uni-directional and bi-directional) which correspond to either Uni-directional RNN or Bi-directional RNN.\n",
    "- `cell_type (self.cell_type)` has three values (simple-rnn, gru, and lstm) which specify the memory cell used in the network.\n",
    "- `embed_model (self.embed_model)` specifes the pretrained Word2Vect model used.\n",
    "-  `embed_size (self.embed_size)` specifes the embedding size. Note that when run_mode is either init-only' or 'init-fine-tune', this embedding size is extracted from embed_model for dimension compatability.\n",
    "- `state_sizes (self.state_sizes)` indicates the list of the hidden sizes  of memory cells. For example, $state\\_sizes = [64, 128]$ means that you have two hidden layers in your network with hidden sizes of $64$ and $128$ respectively.\n",
    "\n",
    "**Complete the code of the class *RNN*.**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[7 points]</span></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_text\n",
    "class RNN:\n",
    "    def __init__(self, run_mode = 'scratch', cell_type= 'gru', network_type = 'uni-directional', embed_model= 'glove-wiki-gigaword-100', \n",
    "                 embed_size= 128, state_sizes = [64, 64], data_manager = None):\n",
    "        self.run_mode = run_mode\n",
    "        self.data_manager = data_manager\n",
    "        self.cell_type = cell_type\n",
    "        self.network_type = network_type\n",
    "        self.state_sizes = state_sizes\n",
    "        self.embed_model = embed_model\n",
    "        self.embed_size = embed_size\n",
    "        if self.run_mode != 'scratch':\n",
    "            self.embed_size = int(self.embed_model.split(\"-\")[-1])\n",
    "        self.data_manager = data_manager\n",
    "        self.vocab_size = dm.vocab_size +1\n",
    "        self.word2idx = dm.word2idx\n",
    "        self.word2vect = None\n",
    "        self.embed_matrix = np.zeros(shape= [self.vocab_size, self.embed_size])\n",
    "    \n",
    "    def build_embedding_matrix(self):\n",
    "        self.word2vect = api.load(self.embed_model) \n",
    "        for word, idx in self.word2idx.items():\n",
    "            try:\n",
    "                self.embed_matrix[idx] = self.word2vect.word_vec(word) \n",
    "            except KeyError:\n",
    "                pass \n",
    "    \n",
    "    @staticmethod\n",
    "    def get_layer(cell_type= 'gru', network_type= 'uni-directional', hidden_size= 128, return_sequences=True, activation = 'tanh'):\n",
    "        if network_type == 'uni-directional':\n",
    "            return UniRNN.get_layer(cell_type=cell_type, state_size=hidden_size, return_sequences=return_sequences) \n",
    "        return BiRNN.get_layer(cell_type=cell_type, state_size=hidden_size, return_sequences=return_sequences)\n",
    "    \n",
    "    def build(self):\n",
    "        x = tf.keras.layers.Input(shape=[None]) \n",
    "        if self.run_mode == \"scratch\":\n",
    "            self.embedding_layer = tf.keras.layers.Embedding(self.vocab_size, self.embed_size, mask_zero=True, trainable=True)\n",
    "        elif self.run_mode == \"init-only\":\n",
    "            self.build_embedding_matrix()\n",
    "            self.embedding_layer = tf.keras.layers.Embedding(self.vocab_size, self.embed_size, mask_zero=True, trainable=False, weights=[self.embed_matrix])\n",
    "        elif self.run_mode == \"init-fine-tune\": \n",
    "            self.build_embedding_matrix()\n",
    "            self.embedding_layer = tf.keras.layers.Embedding(self.vocab_size, self.embed_size, mask_zero=True, trainable=True, weights=[self.embed_matrix]) \n",
    "            \n",
    "        h = self.embedding_layer(x)\n",
    "        num_layers=len(self.state_sizes)\n",
    "        for i in range(num_layers):\n",
    "            if i == num_layers - 1:\n",
    "                h = self.get_layer(cell_type=self.cell_type, hidden_size=self.state_sizes[i], return_sequences=False)(h)\n",
    "            else:\n",
    "                h = self.get_layer(cell_type=self.cell_type, hidden_size=self.state_sizes[i])(h)\n",
    "        h = tf.keras.layers.Dense(dm.num_classes, activation=\"softmax\")(h)\n",
    "        self.model = tf.keras.Model(inputs=x, outputs=h)\n",
    "        \n",
    "    def compile_model(self, *args, **kwargs):\n",
    "        self.model.compile(*args, **kwargs)\n",
    "    \n",
    "    def fit(self, *args, **kwargs):\n",
    "        return self.model.fit(*args, **kwargs)\n",
    "    \n",
    "    def evaluate(self, *args, **kwargs):\n",
    "        self.model.evaluate(*args, **kwargs)       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">**Question 5.3.2**</span> \n",
    "\n",
    "**Design the experiment to compare three running modes. Note that you should stick with fixed values for other attributes and only vary *run_mode*. Give your comments for the results.**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[2 points]</span></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_mode(run_mode):\n",
    "    rnn = RNN(run_mode=run_mode, cell_type='basic_rnn', embed_size=128, state_sizes=[64,128], data_manager=dm)\n",
    "    rnn.build()\n",
    "    opt = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "    rnn.compile_model(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return rnn.fit(dm.tf_train_set.batch(64), epochs=20, validation_data = dm.tf_valid_set.batch(64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "19/19 [==============================] - 2s 51ms/step - loss: 1.0664 - accuracy: 0.6145 - val_loss: 0.5871 - val_accuracy: 0.8227\n",
      "Epoch 2/20\n",
      "19/19 [==============================] - 1s 40ms/step - loss: 0.2425 - accuracy: 0.9376 - val_loss: 0.2771 - val_accuracy: 0.9264\n",
      "Epoch 3/20\n",
      "19/19 [==============================] - 1s 40ms/step - loss: 0.1066 - accuracy: 0.9659 - val_loss: 0.2555 - val_accuracy: 0.9197\n",
      "Epoch 4/20\n",
      "19/19 [==============================] - 1s 40ms/step - loss: 0.0932 - accuracy: 0.9775 - val_loss: 0.1292 - val_accuracy: 0.9498\n",
      "Epoch 5/20\n",
      "19/19 [==============================] - 1s 40ms/step - loss: 0.0289 - accuracy: 0.9950 - val_loss: 0.1095 - val_accuracy: 0.9632\n",
      "Epoch 6/20\n",
      "19/19 [==============================] - 1s 40ms/step - loss: 0.0111 - accuracy: 0.9975 - val_loss: 0.2034 - val_accuracy: 0.9465\n",
      "Epoch 7/20\n",
      "19/19 [==============================] - 1s 40ms/step - loss: 0.0050 - accuracy: 0.9992 - val_loss: 0.1196 - val_accuracy: 0.9565\n",
      "Epoch 8/20\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.1654 - val_accuracy: 0.9532\n",
      "Epoch 9/20\n",
      "19/19 [==============================] - 1s 40ms/step - loss: 7.1029e-04 - accuracy: 1.0000 - val_loss: 0.1436 - val_accuracy: 0.9599\n",
      "Epoch 10/20\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 3.0107e-04 - accuracy: 1.0000 - val_loss: 0.1876 - val_accuracy: 0.9565\n",
      "Epoch 11/20\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 0.0256 - accuracy: 0.9933 - val_loss: 0.2717 - val_accuracy: 0.9398\n",
      "Epoch 12/20\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.1525 - val_accuracy: 0.9498\n",
      "Epoch 13/20\n",
      "19/19 [==============================] - 1s 40ms/step - loss: 1.4167e-04 - accuracy: 1.0000 - val_loss: 0.1483 - val_accuracy: 0.9532\n",
      "Epoch 14/20\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 8.9482e-05 - accuracy: 1.0000 - val_loss: 0.1467 - val_accuracy: 0.9498\n",
      "Epoch 15/20\n",
      "19/19 [==============================] - 1s 40ms/step - loss: 5.8664e-05 - accuracy: 1.0000 - val_loss: 0.1502 - val_accuracy: 0.9498\n",
      "Epoch 16/20\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 3.7314e-05 - accuracy: 1.0000 - val_loss: 0.1579 - val_accuracy: 0.9498\n",
      "Epoch 17/20\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 2.2062e-05 - accuracy: 1.0000 - val_loss: 0.1691 - val_accuracy: 0.9565\n",
      "Epoch 18/20\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 1.1915e-05 - accuracy: 1.0000 - val_loss: 0.1830 - val_accuracy: 0.9565\n",
      "Epoch 19/20\n",
      "19/19 [==============================] - 1s 40ms/step - loss: 5.9596e-06 - accuracy: 1.0000 - val_loss: 0.1996 - val_accuracy: 0.9565\n",
      "Epoch 20/20\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 2.8550e-06 - accuracy: 1.0000 - val_loss: 0.2162 - val_accuracy: 0.9565\n"
     ]
    }
   ],
   "source": [
    "scratch_rnn = test_mode('scratch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\US\\AppData\\Local\\Temp/ipykernel_16492/2139693819.py:24: DeprecationWarning: Call to deprecated `word_vec` (Use get_vector instead).\n",
      "  self.embed_matrix[idx] = self.word2vect.word_vec(word)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 3s 56ms/step - loss: 1.5026 - accuracy: 0.3980 - val_loss: 0.9698 - val_accuracy: 0.6421\n",
      "Epoch 2/20\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 0.9195 - accuracy: 0.6686 - val_loss: 0.6552 - val_accuracy: 0.7893\n",
      "Epoch 3/20\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 0.5964 - accuracy: 0.7985 - val_loss: 0.7343 - val_accuracy: 0.7860\n",
      "Epoch 4/20\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 0.4118 - accuracy: 0.8643 - val_loss: 0.3738 - val_accuracy: 0.8796\n",
      "Epoch 5/20\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 0.2914 - accuracy: 0.9167 - val_loss: 0.3668 - val_accuracy: 0.8796\n",
      "Epoch 6/20\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 0.1662 - accuracy: 0.9534 - val_loss: 0.2555 - val_accuracy: 0.9197\n",
      "Epoch 7/20\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 0.1666 - accuracy: 0.9425 - val_loss: 0.2253 - val_accuracy: 0.9365\n",
      "Epoch 8/20\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 0.1405 - accuracy: 0.9617 - val_loss: 0.2241 - val_accuracy: 0.9331\n",
      "Epoch 9/20\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 0.0674 - accuracy: 0.9833 - val_loss: 0.1745 - val_accuracy: 0.9365\n",
      "Epoch 10/20\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 0.0741 - accuracy: 0.9784 - val_loss: 0.2996 - val_accuracy: 0.9264\n",
      "Epoch 11/20\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 0.0703 - accuracy: 0.9784 - val_loss: 0.1475 - val_accuracy: 0.9465\n",
      "Epoch 12/20\n",
      "19/19 [==============================] - 1s 42ms/step - loss: 0.0680 - accuracy: 0.9775 - val_loss: 0.1423 - val_accuracy: 0.9398\n",
      "Epoch 13/20\n",
      "19/19 [==============================] - 1s 43ms/step - loss: 0.0557 - accuracy: 0.9817 - val_loss: 0.1155 - val_accuracy: 0.9431\n",
      "Epoch 14/20\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 0.0261 - accuracy: 0.9925 - val_loss: 0.1189 - val_accuracy: 0.9532\n",
      "Epoch 15/20\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 0.0848 - accuracy: 0.9775 - val_loss: 0.1308 - val_accuracy: 0.9465\n",
      "Epoch 16/20\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 0.0125 - accuracy: 1.0000 - val_loss: 0.1216 - val_accuracy: 0.9498\n",
      "Epoch 17/20\n",
      "19/19 [==============================] - 1s 41ms/step - loss: 0.0068 - accuracy: 1.0000 - val_loss: 0.1296 - val_accuracy: 0.9498\n",
      "Epoch 18/20\n",
      "19/19 [==============================] - 1s 42ms/step - loss: 0.0363 - accuracy: 0.9917 - val_loss: 0.1514 - val_accuracy: 0.9465\n",
      "Epoch 19/20\n",
      "19/19 [==============================] - 1s 42ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 0.1272 - val_accuracy: 0.9599\n",
      "Epoch 20/20\n",
      "19/19 [==============================] - 1s 43ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.1257 - val_accuracy: 0.9599\n"
     ]
    }
   ],
   "source": [
    "init_rnn = test_mode('init-only')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\US\\AppData\\Local\\Temp/ipykernel_16492/2139693819.py:24: DeprecationWarning: Call to deprecated `word_vec` (Use get_vector instead).\n",
      "  self.embed_matrix[idx] = self.word2vect.word_vec(word)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 2s 54ms/step - loss: 1.5117 - accuracy: 0.3872 - val_loss: 0.9457 - val_accuracy: 0.6388\n",
      "Epoch 2/20\n",
      "19/19 [==============================] - 1s 44ms/step - loss: 0.9098 - accuracy: 0.6753 - val_loss: 0.7343 - val_accuracy: 0.7191\n",
      "Epoch 3/20\n",
      "19/19 [==============================] - 1s 43ms/step - loss: 0.5976 - accuracy: 0.7968 - val_loss: 0.4435 - val_accuracy: 0.8595\n",
      "Epoch 4/20\n",
      "19/19 [==============================] - 1s 44ms/step - loss: 0.3204 - accuracy: 0.8993 - val_loss: 0.3520 - val_accuracy: 0.8930\n",
      "Epoch 5/20\n",
      "19/19 [==============================] - 1s 43ms/step - loss: 0.2084 - accuracy: 0.9301 - val_loss: 0.3351 - val_accuracy: 0.8963\n",
      "Epoch 6/20\n",
      "19/19 [==============================] - 1s 43ms/step - loss: 0.1241 - accuracy: 0.9675 - val_loss: 0.1885 - val_accuracy: 0.9365\n",
      "Epoch 7/20\n",
      "19/19 [==============================] - 1s 43ms/step - loss: 0.1142 - accuracy: 0.9667 - val_loss: 0.2540 - val_accuracy: 0.9264\n",
      "Epoch 8/20\n",
      "19/19 [==============================] - 1s 44ms/step - loss: 0.0627 - accuracy: 0.9808 - val_loss: 0.2041 - val_accuracy: 0.9398\n",
      "Epoch 9/20\n",
      "19/19 [==============================] - 1s 43ms/step - loss: 0.1006 - accuracy: 0.9759 - val_loss: 0.1322 - val_accuracy: 0.9632\n",
      "Epoch 10/20\n",
      "19/19 [==============================] - 1s 43ms/step - loss: 0.0665 - accuracy: 0.9808 - val_loss: 0.1652 - val_accuracy: 0.9532\n",
      "Epoch 11/20\n",
      "19/19 [==============================] - 1s 43ms/step - loss: 0.0502 - accuracy: 0.9875 - val_loss: 0.1881 - val_accuracy: 0.9431\n",
      "Epoch 12/20\n",
      "19/19 [==============================] - 1s 43ms/step - loss: 0.0366 - accuracy: 0.9908 - val_loss: 0.1248 - val_accuracy: 0.9632\n",
      "Epoch 13/20\n",
      "19/19 [==============================] - 1s 43ms/step - loss: 0.0085 - accuracy: 1.0000 - val_loss: 0.1291 - val_accuracy: 0.9565\n",
      "Epoch 14/20\n",
      "19/19 [==============================] - 1s 43ms/step - loss: 0.0411 - accuracy: 0.9925 - val_loss: 0.1264 - val_accuracy: 0.9565\n",
      "Epoch 15/20\n",
      "19/19 [==============================] - 1s 43ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.1304 - val_accuracy: 0.9599\n",
      "Epoch 16/20\n",
      "19/19 [==============================] - 1s 43ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.1585 - val_accuracy: 0.9599\n",
      "Epoch 17/20\n",
      "19/19 [==============================] - 1s 42ms/step - loss: 0.0071 - accuracy: 0.9983 - val_loss: 0.4538 - val_accuracy: 0.8963\n",
      "Epoch 18/20\n",
      "19/19 [==============================] - 1s 43ms/step - loss: 0.0574 - accuracy: 0.9867 - val_loss: 0.0945 - val_accuracy: 0.9666\n",
      "Epoch 19/20\n",
      "19/19 [==============================] - 1s 43ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0884 - val_accuracy: 0.9666\n",
      "Epoch 20/20\n",
      "19/19 [==============================] - 1s 45ms/step - loss: 6.8726e-04 - accuracy: 1.0000 - val_loss: 0.0885 - val_accuracy: 0.9766\n"
     ]
    }
   ],
   "source": [
    "init_fine_rnn = test_mode('init-fine-tune')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">**Question 5.3.3**</span> \n",
    "\n",
    "**Run the above general RNN with at least five parameter sets and try to obtain the best performance. You can stick with the running mode *init-fine-tune* and use grid search to tune other parameters. Record your best model which will be used in the next part.**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[2 points]</span></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Report your results here\n",
    "\n",
    "Model 1 (run_mode ='init-fine-tune',...): accuracy = ...\n",
    "\n",
    "......................................................"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The run of your best model here\n",
    "my_best_rnn = #Insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#0b486b\">5.4. RNNs with Attention for Text and Sequence Classification</span> ###\n",
    "\n",
    "**In what follows, you are required to implement a RNN with the attention machenism for text and sequence classification. This attention mechanism is applied at the last hidden layer of our RNN. Specifically, let $h_1, h_2,...,h_{L-1}, h_L$ be the hidden states at the last hidden layer where $L$ is the sequence length. We compute the context vector $c$ as $c=\\sum_{i=1}^{L}a_{i}h_{i}$ where $a_1,...,a_L$ are the allignment weights (i.e., $a_i\\geq 0$ and $\\sum_{i=1}^{L}a_{i}=1$).**\n",
    "\n",
    "**The allignment weights are computed as follows:**\n",
    "- $a=[a_{i}]_{i=1}^{L}=softmax([s_{i}]_{i=1}^{L})$ where $s= [s_{i}]_{i=1}^{L}$ consists of the allignment scores.\n",
    "- The assigment scores $s= [s_{i}]_{i=1}^{L}$ are computed as $s=tanh(hU)V$ where $h=\\left[\\begin{array}{c}\n",
    "h_{1}\\\\\n",
    "h_{2}\\\\\n",
    "...\\\\\n",
    "h_{L-1}\\\\\n",
    "h_{L}\n",
    "\\end{array}\\right]\\in\\mathbb{R}^{L\\times state\\_size_{3}}$, $U\\in\\mathbb{R}^{state\\_size_{3}\\times output\\_length}$, $V\\in\\mathbb{R}^{output\\_length\\times1}$, and $output\\_length$ is a hyper-parameter. Note that if we consider a mini-batch, the shape of $h$ is $batch\\_size \\times L \\times state\\_size_3$ where $state\\_size_3$ is the hidden size of the last hidden layer.\n",
    "\n",
    "After having the context vector $c$, we concatenate with the last hidden state $h_L$. On top of this concatenation, we conduct the output layer with the softmax activation.\n",
    "\n",
    "<img src=\"./images/attention.png\" align=\"center\" width=1200/>\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red; font-weight:bold\">[Total marks for this part: 14 points]<span></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">**Question 5.4.1**</span>\n",
    "\n",
    "**We declare the  layer `MyAttention` as a class inherited from `tf.keras.layers.Layer` to realize our attention mechanism. You are required to provide the code for this class. Note that in the `def call(self, all_states, last_state)` method, `all_states` is the collection of all hidden states and `last_state` is the last hidden state.**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[5 points]</span></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, output_length= 50):\n",
    "        super(MyAttention, self).__init__()\n",
    "        self.called = False \n",
    "        self.output_length = output_length\n",
    "    \n",
    "    #all_states is the collection of all hidden states and last_state is the last hidden state\n",
    "    def call(self, all_states, last_state):\n",
    "        try:\n",
    "            print(\"Called\")\n",
    "            if not self.called: \n",
    "                print(\"Adding Weight\")\n",
    "                print(\"Last State\", last_state.shape)\n",
    "                print(\"Allstate shape\", all_states.shape)\n",
    "                U_shape = last_state.shape + (self.output_length, ) \n",
    "                print(\"U_shape\", U_shape)\n",
    "                self.U = self.add_weight(\"U\",shape=(128,), trainable=True)\n",
    "                self.V = self.add_weight(\"V\", shape=(self.output_length, 1), trainable=True)\n",
    "                print(\"U shape\", self.U.shape)\n",
    "                print(\"V shape\", self.V.shape)\n",
    "            self.called = True \n",
    "            print(\"All States shape\", all_states.shape)\n",
    "            s = tf.keras.activations.tanh(all_states @ self.U) @ self.V \n",
    "            return tf.keras.activations.softmax(s)\n",
    "        except Exception as e: \n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">**Question 5.4.2**</span> \n",
    "\n",
    "**You are required to extend the class `RNN` in Section `5.3.1` to achieve the class `Attention_RNN` in which the attention mechanism mentioned above is applied at the last hidden layer.**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[5 points]</span></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention_RNN:\n",
    "    def __init__(self, run_mode = 'scratch', cell_type= 'gru', network_type = 'uni-directional', embed_model= 'glove-wiki-gigaword-100', \n",
    "                 embed_size= 128, state_sizes = [64, 64], data_manager = None):\n",
    "        self.run_mode = run_mode\n",
    "        self.data_manager = data_manager\n",
    "        self.cell_type = cell_type\n",
    "        self.network_type = network_type\n",
    "        self.state_sizes = state_sizes\n",
    "        self.embed_model = embed_model\n",
    "        self.embed_size = embed_size\n",
    "        if self.run_mode != 'scratch':\n",
    "            self.embed_size = int(self.embed_model.split(\"-\")[-1])\n",
    "        self.data_manager = data_manager\n",
    "        self.vocab_size = dm.vocab_size +1\n",
    "        self.word2idx = dm.word2idx\n",
    "        self.word2vect = None\n",
    "        self.embed_matrix = np.zeros(shape= [self.vocab_size, self.embed_size])\n",
    "    \n",
    "    def build_embedding_matrix(self):\n",
    "        self.word2vect = api.load(self.embed_model) \n",
    "        for word, idx in self.word2idx.items():\n",
    "            try:\n",
    "                self.embed_matrix[idx] = self.word2vect.word_vec(word) \n",
    "            except KeyError:\n",
    "                pass \n",
    "    \n",
    "    @staticmethod\n",
    "    def get_layer(cell_type= 'gru', network_type= 'uni-directional', hidden_size= 128, return_sequences=True, activation = 'tanh'):\n",
    "        if network_type == 'uni-directional':\n",
    "            return UniRNN.get_layer(cell_type=cell_type, state_size=hidden_size, return_sequences=return_sequences) \n",
    "        return BiRNN.get_layer(cell_type=cell_type, state_size=hidden_size, return_sequences=return_sequences)\n",
    "    \n",
    "    def build(self):\n",
    "        x = tf.keras.layers.Input(shape=[None]) \n",
    "        if self.run_mode == \"scratch\":\n",
    "            self.embedding_layer = tf.keras.layers.Embedding(self.vocab_size, self.embed_size, mask_zero=True, trainable=True)\n",
    "        elif self.run_mode == \"init-only\":\n",
    "            self.build_embedding_matrix()\n",
    "            self.embedding_layer = tf.keras.layers.Embedding(self.vocab_size, self.embed_size, mask_zero=True, trainable=False, weights=[self.embed_matrix])\n",
    "        elif self.run_mode == \"init-fine-tune\": \n",
    "            self.build_embedding_matrix()\n",
    "            self.embedding_layer = tf.keras.layers.Embedding(self.vocab_size, self.embed_size, mask_zero=True, trainable=True, weights=[self.embed_matrix]) \n",
    "            \n",
    "        h = self.embedding_layer(x)\n",
    "        num_layers=len(self.state_sizes)\n",
    "        for i in range(num_layers):\n",
    "            if i == num_layers - 1:\n",
    "                last_state = self.get_layer(cell_type=self.cell_type, hidden_size=self.state_sizes[i], return_sequences=False)(h)\n",
    "                all_state = self.get_layer(cell_type=self.cell_type, hidden_size=self.state_sizes[i], return_sequences=True)(h)\n",
    "            else:\n",
    "                h = self.get_layer(cell_type=self.cell_type, hidden_size=self.state_sizes[i])(h)\n",
    "        attention = MyAttention()(all_state, last_state)\n",
    "        h = tf.keras.layers.Dense(dm.num_classes, activation=\"softmax\")(last_state)\n",
    "        self.model = tf.keras.Model(inputs=x, outputs=h)\n",
    "        \n",
    "    def compile_model(self, *args, **kwargs):\n",
    "        self.model.compile(*args, **kwargs)\n",
    "    \n",
    "    def fit(self, *args, **kwargs):\n",
    "        return self.model.fit(*args, **kwargs)\n",
    "    \n",
    "    def evaluate(self, *args, **kwargs):\n",
    "        self.model.evaluate(*args, **kwargs)       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">**Question 5.4.3**</span> \n",
    "\n",
    "**Choose a common setting for standard RNN and RNN with attention and conduct experiments to compare them. A setting here means `cell_type`, `network_type`, and list of `state sizes`**.\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[4 points]</span></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Called\n",
      "Adding Weight\n",
      "Last State (None, 128)\n",
      "Allstate shape (None, None, 128)\n",
      "U_shape (None, 128, 50)\n",
      "U shape (128,)\n",
      "V shape (50, 1)\n",
      "All States shape (None, None, 128)\n",
      "Shape must be at least rank 2 but is rank 1 for '{{node my_attention_33/matmul}} = BatchMatMulV2[T=DT_FLOAT, adj_x=false, adj_y=false](Placeholder, my_attention_33/matmul/ReadVariableOp)' with input shapes: [?,?,128], [128].\n",
      "Epoch 1/20\n",
      "19/19 [==============================] - 2s 61ms/step - loss: 1.0011 - accuracy: 0.6719 - val_loss: 0.5159 - val_accuracy: 0.8796\n",
      "Epoch 2/20\n",
      "19/19 [==============================] - 1s 50ms/step - loss: 0.2590 - accuracy: 0.9292 - val_loss: 0.2371 - val_accuracy: 0.9298\n",
      "Epoch 3/20\n",
      "13/19 [===================>..........] - ETA: 0s - loss: 0.1248 - accuracy: 0.9615"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16492/627787088.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mopt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRMSprop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mrnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mopt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'sparse_categorical_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mrnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtf_train_set\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtf_valid_set\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16492/1390931393.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\US\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\US\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1562\u001b[0m                         ):\n\u001b[0;32m   1563\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1564\u001b[1;33m                             \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1565\u001b[0m                             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1566\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\US\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\US\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\US\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    945\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    948\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\US\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2494\u001b[0m       (graph_function,\n\u001b[0;32m   2495\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2497\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2498\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\US\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1860\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1861\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1862\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1863\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1864\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mc:\\Users\\US\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    500\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\US\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Most likely need to work with correct sizes\n",
    "rnn = Attention_RNN(run_mode='scratch', cell_type='basic_rnn', embed_size=128, state_sizes=[64,128], data_manager=dm)\n",
    "rnn.build()\n",
    "opt = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "rnn.compile_model(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "rnn.fit(dm.tf_train_set.batch(64), epochs=20, validation_data = dm.tf_valid_set.batch(64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#0b486b\">5.5. Investigating the embedding vectors from the embedding matrix</span> ###\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red; font-weight:bold\">[Total marks for this part: 10 points]<span></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As you know, the embedding matrix is a collection of embedding vectors, each is for one word. In this part, you will base on the cosine similarity of the embedding vectors for the words to find the top-k most relevant words for a given word.**\n",
    "\n",
    "**Good embeddings should have words close in meaning near each other by some similarity metrics. The similarity metric we'll use is the `consine` similarity, which is defined for two vector $\\mathbf{u}$ and $\\mathbf{v}$ as $\\cos(\\mathbf{u}, \\mathbf{v})=\\frac{\\mathbf{u} \\cdot \\mathbf{v}}{\\left\\Vert{\\mathbf{u}}\\right\\Vert\\left\\Vert{\\mathbf{v}}\\right\\Vert}$ where $\\cdot$ means dot product and $\\left\\Vert\\cdot\\right\\Vert$ means the $\\mathcal{L}_2$ norm.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(u,v):\n",
    "    return np.dot(u,v)/(np.linalg.norm(u)*np.linalg.norm(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">**Question 5.5.1**</span> \n",
    "\n",
    "**You are required to write the code for the function `find_most_similar(word= None, k=5, model= None)`. As its name, this function returns the `top-k most relevant word` for a given word based on the cosine similarity of the embedding vectors.**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[5 points]</span></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_similar(word= None, k=5, model= None):\n",
    "    try:\n",
    "        #Insert your code here, need to find gensim\n",
    "        pass\n",
    "    except: #word not in the vocabulary\n",
    "        print(\"Word is not in the dictionary!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the example of the above function. As you can observe, the result makes sense which demonstrates that we obtain a good model with the meaningful embedding matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_most_similar(word='poland', k=10, model= my_best_rnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">**Question 5.5.2**</span> \n",
    "​\n",
    "**You are required to write the code for the function `plot3D_with_labels(word= None, model= None, k= 10)`. As its name, this function visualizes the `top-k most relevant word` for a given word based on the cosine similarity of the embedding vectors by using tSNE to project the embedding vectors to a 3D space.**\n",
    "​\n",
    "\n",
    "<img src=\"./images/3D_plots.png\" align=\"center\" width=600/>\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[5 points]</span></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(perplexity=30, n_components=3, init='pca', n_iter=5000)\n",
    "def plot3D_with_labels(word= None, model= None, k= 10):\n",
    "   #Insert yout codeh here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot3D_with_labels(word='poland', k=20, model= rnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "<div style=\"text-align: center\"> <span style=\"color:green\">GOOD LUCK WITH YOUR ASSIGNMENT 2!</span> </div>\n",
    "<div style=\"text-align: center\"> <span style=\"color:black\">END OF ASSIGNMENT</span> </div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "b916ba2c14536b214077dfec6c206136f3cd9dd298193f9fd8924ac064eebdc3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
