{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#0b486b\">  FIT5215: Deep Learning (Summer Semester A 2021)</span>\n",
    "***\n",
    "*CE/Lecturer:*  **Dr Trung Le** | trunglm@monash.edu <br/>\n",
    "*Head TA:*  **Mr Tuan Nguyen**  \\[tuan.nguyen2@monash.edu \\] <br/>\n",
    "*Tutor:* **Dr Binh Nguyen** \\[binh.nguyen1@monash.edu\\] | **Mr Thanh Nguyen** \\[thanh.nguyen4@monash.edu\\]\n",
    "<br/> <br/>\n",
    "Faculty of Information Technology, Monash University, Australia\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#0b486b\">Tutorial 07c (Additional Reading): RNN for Text Generation</span> <span style=\"color:red\">***</span> #\n",
    "\n",
    "This tutorial is designed to show one of the applications of RNN in generating texts or sequences. Basically, we train an RNN using the maximum log-likelihood principle and then use this trained RNN to generate texts that imitate the existed texts in the dataset we trained our RNN on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first import the necessary modules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#0b486b\">I. Download and preprocess data</span> ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import shutil\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \".\"\n",
    "CHECKPOINT_DIR = os.path.join(DATA_DIR, \"checkpoints\")\n",
    "if not os.path.exists(CHECKPOINT_DIR):\n",
    "    os.mkdir(CHECKPOINT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below function helps to download the dataset at a specific URL and split the sentences into characters.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_read(urls):\n",
    "    texts = []\n",
    "    for i, url in enumerate(urls):\n",
    "        p = tf.keras.utils.get_file(\"ex1-{:d}.txt\".format(i), url, cache_dir=\".\")\n",
    "        text = open(p, \"r\", encoding=\"utf8\").read()\n",
    "        # remove byte order mark\n",
    "        text = text.replace(\"\\ufeff\", \"\")\n",
    "        # remove newlines\n",
    "        text = text.replace('\\n', ' ')\n",
    "        text = re.sub(r'\\s+', \" \", text)\n",
    "        # add it to the list\n",
    "        texts.extend(text)\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We download the dataset and the variable *texts* is a list containing all characters of the sentences in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = download_and_read([\"http://www.gutenberg.org/cache/epub/28885/pg28885.txt\", \"https://www.gutenberg.org/files/12/12-0.txt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['P', 'r', 'o', 'j', 'e', 'c', 't', ' ', 'G', 'u', 't', 'e', 'n', 'b', 'e', 'r', 'g', \"'\", 's', ' ', 'A', 'l', 'i', 'c', 'e', \"'\", 's', ' ', 'A', 'd', 'v', 'e', 'n', 't', 'u', 'r', 'e', 's', ' ', 'i', 'n', ' ', 'W', 'o', 'n', 'd', 'e', 'r', 'l', 'a', 'n', 'd', ',', ' ', 'b', 'y', ' ', 'L', 'e', 'w', 'i', 's', ' ', 'C', 'a', 'r', 'r', 'o', 'l', 'l', ' ', 'T', 'h', 'i', 's', ' ', 'e', 'B', 'o', 'o', 'k', ' ', 'i', 's', ' ', 'f', 'o', 'r', ' ', 't', 'h', 'e', ' ', 'u', 's', 'e', ' ', 'o', 'f', ' ']\n"
     ]
    }
   ],
   "source": [
    "print(texts[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract the vocabulary of all unique characters in this dataset and store in *vocab*. In addition, we have two dictionaries: *char2idx* and *idx2char* to convert between the characters and their indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 90\n"
     ]
    }
   ],
   "source": [
    "# create the vocabulary\n",
    "vocab = sorted(set(texts))\n",
    "print(\"vocab size: {:d}\".format(len(vocab)))\n",
    "# create mapping from vocab chars to ints\n",
    "char2idx = {c:i for i, c in enumerate(vocab)}\n",
    "idx2char = {i:c for c, i in char2idx.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We transform the characters in *texts* to the indices in *texts_as_ints* and then make a Tensorflow dataset *data* from this *texts_as_ints*. Finally, we chop *data* into batch dataset *sequences*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numericize the texts\n",
    "texts_as_ints = np.array([char2idx[c] for c in texts])\n",
    "data = tf.data.Dataset.from_tensor_slices(texts_as_ints)\n",
    "# number of characters to show before asking for prediction\n",
    "# sequences: [None, 100]\n",
    "seq_length = 100\n",
    "sequences = data.batch(seq_length + 1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the below function, you can imagine *sequence* is a batch of characters, for example \\['I', 'l', 'o', 'v', 'e', 'D', 'L'\\], this function will return \\['I', 'l', 'o', 'v', 'e', 'D'\\] and \\['l', 'o', 'v', 'e', 'D', 'L'\\].\n",
    "\n",
    "The idea later is that we feed \\['I', 'l', 'o', 'v', 'e', 'D'\\] to our RNN and try to predict \\['l', 'o', 'v', 'e', 'D', 'L'\\] which is the set of next characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_labels(sequence):\n",
    "    input_seq = sequence[0:-1]\n",
    "    output_seq = sequence[1:]\n",
    "    return input_seq, output_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now apply the function *split_train_labels* to each batch in sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function split_train_labels at 0x00000207563ABD30> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function split_train_labels at 0x00000207563ABD30> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "sequences = sequences.map(split_train_labels)\n",
    "# set up for training\n",
    "# batches: [None, 64, 100]\n",
    "batch_size = 64\n",
    "steps_per_epoch = len(texts) // seq_length // batch_size\n",
    "dataset = sequences.shuffle(10000).batch(batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We encapsulate our generation model in the class *CharGenModel*. Our model has one embedding layer and one hidden layer with GRU cells. Note that we need to set *return_sequences=True* for the hidden layer so that it returns a 3D tensor of all hidden values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharGenModel(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, **kwargs):\n",
    "        super(CharGenModel, self).__init__(**kwargs)\n",
    "        self.embedding_layer = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn_layer = tf.keras.layers.GRU(embedding_dim, recurrent_initializer=\"glorot_uniform\", recurrent_activation=\"sigmoid\", \n",
    "                                             stateful=True, return_sequences=True)\n",
    "        self.dense_layer = tf.keras.layers.Dense(vocab_size)\n",
    "    \n",
    "    def call(self, x):\n",
    "        x = self.embedding_layer(x)\n",
    "        x = self.rnn_layer(x)\n",
    "        x = self.dense_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "embedding_dim = 256\n",
    "rnn_output_dim = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CharGenModel(vocab_size, embedding_dim)\n",
    "model.build(input_shape=(batch_size, None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the loss function which is the sum of the loss at each time step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(labels, predictions):\n",
    "    return tf.losses.sparse_categorical_crossentropy(labels,predictions,from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.optimizers.Adam(), loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate a text, we start from a prefix_string. We convert this string to a list of indices and declare a 2D tensor from this list with the first dimension to be $1$. We feed *inputs* to the model to work out the prediction probability *preds* and sample *pred_id* from this probability and so on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, prefix_string, char2idx, idx2char, num_chars_to_generate=1000, temperature=1.0):\n",
    "    inputs = [char2idx[s] for s in prefix_string]\n",
    "    inputs = tf.expand_dims(inputs, 0)\n",
    "    text_generated = []\n",
    "    model.reset_states()\n",
    "    for i in range(num_chars_to_generate):\n",
    "        preds = model(inputs)\n",
    "        preds = tf.squeeze(preds, 0) / temperature\n",
    "        # predict char returned by model\n",
    "        pred_id = tf.random.categorical(preds, num_samples=1)[-1, 0].numpy()\n",
    "        text_generated.append(idx2char[pred_id])\n",
    "        # pass the prediction as the next input to the model\n",
    "        inputs = tf.expand_dims([pred_id], 0)\n",
    "    return prefix_string + \"\".join(text_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "54/54 [==============================] - 6s 115ms/step - loss: 3.3376\n",
      "Epoch 2/10\n",
      "54/54 [==============================] - 6s 120ms/step - loss: 2.6035\n",
      "Epoch 3/10\n",
      "54/54 [==============================] - 7s 127ms/step - loss: 2.3687\n",
      "Epoch 4/10\n",
      "54/54 [==============================] - 7s 122ms/step - loss: 2.2286\n",
      "Epoch 5/10\n",
      "54/54 [==============================] - 7s 127ms/step - loss: 2.1081\n",
      "Epoch 6/10\n",
      "54/54 [==============================] - 6s 118ms/step - loss: 2.0036\n",
      "Epoch 7/10\n",
      "54/54 [==============================] - 7s 128ms/step - loss: 1.9093\n",
      "Epoch 8/10\n",
      "54/54 [==============================] - 7s 122ms/step - loss: 1.8328\n",
      "Epoch 9/10\n",
      "54/54 [==============================] - 7s 122ms/step - loss: 1.7586\n",
      "Epoch 10/10\n",
      "54/54 [==============================] - 7s 130ms/step - loss: 1.7002\n",
      "Alice said _VETM 1.8...3 pakent Moancipalar: Is you agay what he round the took. ‘I wonder! He won,\" the Parger look, add,’ said to may!’ And I Soil_ it so mary ow wheren; \"Of, it kept \"Theally hears pad inching take ramping inishes wouldn't hear--thingenat to cear upoo saiding the Cather, but sive said theire hax!\" said the bitited in a contly sirig, And plitenfer; 'sh piftice head ot to, the Kintt ever detton you onlud!\" \"Ho thay What deep very didn's a pating as impige. (that chuther, in a repurn any_'s well, ho itter: and whole rase! I couldn’t a woud to you know it of of purither to you more suid, (wo_ it his saids, all Praca kither it comply restiself ree, of co sisher a glasce said ne voide this dightere which pursack it in I shat wilk futting. ‘I. [Sow thisk of she can no beine with the larg helys looking her thy hang is ganally wink at madun a that the heatled for see Hattleededyt up. She time tre, it farring numpint indopations as she remong. ‘you'll nextion. ‘That oncerse: it was,\n",
      "---\n",
      "Epoch 1/10\n",
      "54/54 [==============================] - 7s 121ms/step - loss: 1.6493\n",
      "Epoch 2/10\n",
      "54/54 [==============================] - 7s 121ms/step - loss: 1.6009\n",
      "Epoch 3/10\n",
      "54/54 [==============================] - 7s 125ms/step - loss: 1.5614\n",
      "Epoch 4/10\n",
      "54/54 [==============================] - 7s 125ms/step - loss: 1.5255\n",
      "Epoch 5/10\n",
      "54/54 [==============================] - 7s 128ms/step - loss: 1.4940\n",
      "Epoch 6/10\n",
      "54/54 [==============================] - 7s 122ms/step - loss: 1.4668\n",
      "Epoch 7/10\n",
      "54/54 [==============================] - 7s 122ms/step - loss: 1.4377\n",
      "Epoch 8/10\n",
      "54/54 [==============================] - 7s 125ms/step - loss: 1.4161\n",
      "Epoch 9/10\n",
      "54/54 [==============================] - 7s 123ms/step - loss: 1.3948\n",
      "Epoch 10/10\n",
      "54/54 [==============================] - 6s 120ms/step - loss: 1.3728\n",
      "Alice queer of to great put _fervisiop, and thes got I diddle Made offered the Rabbit know your backagates with a losh very claincime,\" said Alice. ‘You gron The pubsed to Alice, said the jury Looking, just as All usoned tryinall your busided to for this eBQueed round an then _yruedled of forect to have a canagh them.\" \"The carrelly, shere afre listo any be more the King. Ih, Tweedle; with a heap. So they was to jurm, The gardenting sire!’ the Tweems,’ she knoely, ‘Ancily if she repeate, in a morever the bleate.\" There wattenly caviver! But see having off eventh of my inning it: and the caller not me longs. Staying herself was sort. Some were some upon and marked and the tried to swe could suppoce of pinnce inderself statily like a dolitily you agreehed now, and stos now Alice had lesten took answers couldn’t she was a branchos by be sive it is a chare. Alice Twould, she mumbly. ‘I don’t such a bong-tasing you?’ said Alice, for htracome of the diened to be sawial only joss at one ry many wit\n",
      "---\n",
      "Epoch 1/10\n",
      "54/54 [==============================] - 6s 117ms/step - loss: 1.3568\n",
      "Epoch 2/10\n",
      "54/54 [==============================] - 7s 137ms/step - loss: 1.3392\n",
      "Epoch 3/10\n",
      "54/54 [==============================] - 7s 131ms/step - loss: 1.3243\n",
      "Epoch 4/10\n",
      "54/54 [==============================] - 7s 123ms/step - loss: 1.3070\n",
      "Epoch 5/10\n",
      "54/54 [==============================] - 7s 122ms/step - loss: 1.2916\n",
      "Epoch 6/10\n",
      "54/54 [==============================] - 7s 127ms/step - loss: 1.2816\n",
      "Epoch 7/10\n",
      "54/54 [==============================] - 7s 124ms/step - loss: 1.2645\n",
      "Epoch 8/10\n",
      "54/54 [==============================] - 7s 126ms/step - loss: 1.2573\n",
      "Epoch 9/10\n",
      "54/54 [==============================] - 7s 120ms/step - loss: 1.2424\n",
      "Epoch 10/10\n",
      "54/54 [==============================] - 7s 122ms/step - loss: 1.2258\n",
      "Alice just of creat at ht fround cauts mad lustide bainthed groment, the Falling right as she is a reason the rige of disceidud his so-never three door long orner are of Mouse-traws was they were only a book. ‘As tears and the Red Queen: \"but that is the callection!\" said Alice. \"I haven't like the work very asked in expresettillish sitting up,’ the Gnat’s Alice asked. ‘I be an explanabout-- And white others, as it was played at it.\" \"She means the sheep about down,” so was a sudder to the eather was a voice jumping up into the Project Gutenberg _That's I'm got exactly word quite tot the pinch to one a large gatched it.\" \"The cossitting theose he disty that the Queen, makes taken to the people was both it,’ said Alice, very door,\" said Alice. ‘Then it was, if Alice we life come up some mind of the cralled back volunteers from here’s no very made?’ ‘Never. Moss such surpris indight to eye the Queen). \"Dinahal donsters coming in a LAND BUTT on itself,\" (she hew myself. As come in her meach of \n",
      "---\n",
      "Epoch 1/10\n",
      "54/54 [==============================] - 7s 125ms/step - loss: 1.2204\n",
      "Epoch 2/10\n",
      "54/54 [==============================] - 7s 123ms/step - loss: 1.2078\n",
      "Epoch 3/10\n",
      "54/54 [==============================] - 6s 119ms/step - loss: 1.1981\n",
      "Epoch 4/10\n",
      "54/54 [==============================] - 6s 116ms/step - loss: 1.1857\n",
      "Epoch 5/10\n",
      "54/54 [==============================] - 6s 120ms/step - loss: 1.1771\n",
      "Epoch 6/10\n",
      "54/54 [==============================] - 6s 116ms/step - loss: 1.1619\n",
      "Epoch 7/10\n",
      "54/54 [==============================] - 6s 117ms/step - loss: 1.1582\n",
      "Epoch 8/10\n",
      "54/54 [==============================] - 6s 118ms/step - loss: 1.1482\n",
      "Epoch 9/10\n",
      "54/54 [==============================] - 7s 124ms/step - loss: 1.1370\n",
      "Epoch 10/10\n",
      "54/54 [==============================] - 7s 122ms/step - loss: 1.1300\n",
      "Alice Fanbertamed into the good heavy beaking it:--iat laughed “AM-- Next, \"You MOUDARAY, Pray), I darked up my nden. ‘Filled in her buttons.\" Which was complying, with it down again, for ah which the poor MELead of watching up anything!\" said Alice, to Pexplay! Why, it’s a volct, then feeling in all right,’ said the King.’ ‘All rathis winks and brong to the face in the ansolething outside; but thesh stops.’ ‘Yet's all: In Sweedly, the rohes like that she'd been she. Let very gate the jound earities, and because how were saying, full may be again. I mean,’ he went up in aftee to see where I was half a feep hive must begin such a thing! And she thought!\" said the Caterpillar. Alice full of breaking a pitione when you’re talking to her, forth in paragraph 1.E. Ertantle of it to herself, as the marked with a thing, timidly, tusts:--unly heard a sury with by and back and impeys grezed to herself, now _I_ grin blew for tealsily feel at all talked to mind. ‘It’s a Lows): well be a puzzled out here\n",
      "---\n",
      "Epoch 1/10\n",
      "54/54 [==============================] - 6s 116ms/step - loss: 1.1205\n",
      "Epoch 2/10\n",
      "54/54 [==============================] - 6s 117ms/step - loss: 1.1096\n",
      "Epoch 3/10\n",
      "54/54 [==============================] - 7s 126ms/step - loss: 1.1033\n",
      "Epoch 4/10\n",
      "54/54 [==============================] - 7s 125ms/step - loss: 1.0938\n",
      "Epoch 5/10\n",
      "54/54 [==============================] - 7s 120ms/step - loss: 1.0864\n",
      "Epoch 6/10\n",
      "54/54 [==============================] - 7s 124ms/step - loss: 1.0772\n",
      "Epoch 7/10\n",
      "54/54 [==============================] - 6s 120ms/step - loss: 1.0696\n",
      "Epoch 8/10\n",
      "54/54 [==============================] - 7s 131ms/step - loss: 1.0596\n",
      "Epoch 9/10\n",
      "54/54 [==============================] - 6s 119ms/step - loss: 1.0504\n",
      "Epoch 10/10\n",
      "54/54 [==============================] - 7s 121ms/step - loss: 1.0472\n",
      "Alice CARRENTERES” you know,’ she cried, “dive without croplice-golonally crab-cauld in the amplomed, and shouting, perfection off this right his), Kitty, and to leave out hes, so. There's had anyone with: then he left of works in great day considering as well come from.’ This suddenly that Alice is, till yourself as--to get in at the knoow her own my plenty of pleasang terms widds.’ ‘I am not! How do you’ll a subject of sight of a copyright hand, by oartice very much must be, twimblestrobody would become?’ she said to Alice, and the next began maded her ears!’ ‘I don’t know what the rubs?’ Alice falling all over sien, then, in another mout delass format shown and before, mivided the ndarce of the name. In additions. 1.E.1 Project Gutenberg-tm electronic work,\" said the Carpertf Project Gutenberg. 1.S. Poo old. \"ED ME. ‘I’ll more surp as commired by your Willas; she had such a Caucup. You live lith so mouse--'OURLAGY THE PROJHT, GUTENBERG--my followed again, and Alice knew it 'D WOUKL cause.\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # FATAL\n",
    "logging.getLogger('tensorflow').setLevel(logging.FATAL)\n",
    "\n",
    "num_epochs = 50\n",
    "for i in range(num_epochs // 10):\n",
    "    model.fit(dataset.repeat(), epochs=10, steps_per_epoch=steps_per_epoch)\n",
    "    checkpoint_file = os.path.join(CHECKPOINT_DIR, \"model_epoch_{:d}\".format(i+1))\n",
    "    model.save_weights(checkpoint_file)\n",
    "    gen_model = CharGenModel(vocab_size, embedding_dim)\n",
    "    gen_model.load_weights(checkpoint_file)\n",
    "    gen_model.build(input_shape=(1, None))\n",
    "    # create generative model using the trained model so far\n",
    "    print(generate_text(gen_model, \"Alice \", char2idx, idx2char))\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### <span style=\"color:#0b486b\"> <div  style=\"text-align:center\">**THE END**</div> </span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2.5",
   "language": "python",
   "name": "tf2.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
