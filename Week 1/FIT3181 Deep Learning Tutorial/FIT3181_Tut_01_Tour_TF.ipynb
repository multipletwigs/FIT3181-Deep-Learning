{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#0b486b\">  FIT3181: Deep Learning (2022)</span>\n",
    "***\n",
    "*CE/Lecturer (Clayton):*  **Dr Trung Le** | trunglm@monash.edu <br/>\n",
    "*Lecturer (Malaysia):*  **Dr Lim Chern Hong** | lim.chernhong@monash.edu <br/>  <br/>\n",
    "*Tutor:*  **Mr Thanh Nguyen** \\[Thanh.Nguyen4@monash.edu \\] |**Mr Tuan Nguyen**  \\[tuan.ng@monash.edu \\] |**Mr Anh Bui** \\[tuananh.bui@monash.edu\\] | **Dr Binh Nguyen** \\[binh.nguyen1@monash.edu \\] | **Mr Md Mohaimenuzzaman** \\[md.mohaimen@monash.edu \\] |**Mr James Tong** \\[james.tong1@monash.edu \\]\n",
    "<br/> <br/>\n",
    "Faculty of Information Technology, Monash University, Australia\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#0b486b\">Tutorial 1: Deep Learning with TensorFlow</span> #\n",
    "**The purpose of this tutorial is to demonstrate how to work with an open source software library for developing deep neural networks apllications, called TensorFlow (TF). In this tutorial, following topics are covered:**\n",
    "\n",
    "1. A quick tour of TensorFlow 1.x\n",
    "2. How to visualize a computational graph and its node values using Tensorboard.\n",
    "3. Advancements of TensorFlow 2.x in comparison to TensorFlow 1.x.\n",
    "\n",
    "**Part I of this tutorial introduces TF 1.x. Although it is much more convenient if we build up and train deep learning models in TF 2.x, TF 1.x serves as a building block for TF 2.x, which can be viewed as a wrapper of TF 1.x. Therefore, understanding how TF 1.x operates is crucial and supports us in diving deeper into TF 2.x. In addition, the concept of computational graph with computational nodes (e.g., constants, variables, and place holders) is central of both TF 1.x and 2.x though being encapsulated in TF 2.x. Furthermore, when you join in the job marker, many of legacy codes have been written in TF 1.x. Therefore, it is crucial to have ideas of how TF 1.x generally operate.**\n",
    "\n",
    "**Part II introduces TF 2.x, a framework built up on top of TF 1.x. It is much simpler to use and deploy compared to TF 1.x. Most of latter advanced deep learning models in this unit will be implemented using TF 2.x.**\n",
    "\n",
    "**In addition, to facilitate the study of the students, the tutorial content also has additional material to indicate how the TF 1.x code can be ported to that of TF 2.x.** \n",
    "\n",
    "**References and additional reading and resources**\n",
    "- [Installing Tensorflow on Windows](https://www.tensorflow.org/install)\n",
    "- [Tensorflow API documentations](https://www.tensorflow.org/api_docs)\n",
    "- [Examples with Tensorflow](https://www.tensorflow.org/tutorials)\n",
    "\n",
    "**Acknowledgement**: *Some materials used in this tutorial have been adapted from Chapter 3 of the the book \"Learning TensorFlow: A Guide to Building Deep Learning Systems\" by Hope, Resheff and Lieder and Chapter 2 of \"Deep Learning with with TensorFlow 2 and Keras\" by Antonio Culli, Amita Kapoor, Sujit Pal.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#0b486b\"> I. A quick tour of TensorFlow 1.x </span>  <span style=\"color:red\">***** (highly important)</span>\n",
    "\n",
    "### Table of Content\n",
    "I.1 Computational Graph <br>\n",
    "I.2 Declare a Computational Graph  <br>\n",
    "I.3 Fetches Values  <br>\n",
    "I.4 More about Graph  <br>\n",
    "I.5 Data Types and Cast  <br>\n",
    "I.6 Get shape  <br>\n",
    "I.7 Initialize Tensors <br>\n",
    "I.8 Matrix Multiplication and Activation  <br>\n",
    "I.9 Name <br>\n",
    "I.10 Name Scope <br>\n",
    "I.11 Variable <br>\n",
    "I.12 Variable Scope and Reuse Variable <br>\n",
    "I.13 Name Scope & Variable Scope <br>\n",
    "I.14 Placeholder <br>\n",
    "I.15 Save and Restore Models <br>\n",
    "I.16 Visualization with TensorBoard <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#0b486b\"> I.1 Computational Graph </span> ###\n",
    "Computational graph is a basic concept of TensorFlow which represents the functional dependency of nodes. The following figure presents a typical computational graph wherein each node is a tensor. <br/>\n",
    "\n",
    "<img src=\"images/ComputationalGraph.png\" width=\"500\" align=\"center\"/> \n",
    "\n",
    "The above computational graph presents some functional dependencies:\n",
    "\n",
    "- $c$ is dependent on $a$, $b$.\n",
    "- $d$ depends on $a$, $e$ depends on $c$.\n",
    "- $f$ depends on $d$, $e$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#0b486b\"> I.2 Declare a Computational Graph </span>\n",
    "The following code snippet shows how to declare a computational graph in TensorFlow. Once we import TF, a specific `empty default graph` is formed. All the nodes we create are automatically associated with that default graph. It is worth noting that we just `declare a graph` and **nothing** has been executed yet.\n",
    "\n",
    "Below we import TensorFlow 2.x. To ensure that this is comparatible and works with TF 1.x codes, we need to import the coressponding package `tensorflow.compat.v1`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\Work Stuff\\Monash Stuff\\Year 3 Semester 1\\FIT3181 Deep Learning\\FIT3181-Deep-Learning\\Week 1\\wk1\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant(5)\n",
    "b = tf.constant(2)\n",
    "c = tf.constant(3)\n",
    "d = tf.multiply(a,b)\n",
    "e = tf.add(b,c)\n",
    "f = tf.subtract(d,e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color:red\">Exercise 1</span>**: Draw the computational graph of the above computational process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#0b486b\"> I.3 Fetches Values </span> ###\n",
    "To query the nodes in a computational graph, we create a `session` and run a query in this sesstion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outs=[5, 2, 3, 10, 5, 5]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    fetches = [a,b,c,d,e,f] # First you need to fetch all the sessions\n",
    "    outs = sess.run(fetches) \n",
    "    print(\"outs={}\".format(outs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color:red\">Exercise 2</span>**: Write code in the cell below to create a new node $g = a^2 + (b * f)$ and $h= (b+c)*d + e^2*c$, create a session, run a query to compute and display the values of $g$ and $h$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code here\n",
    "g = tf.add(tf.pow(a, 2), tf.multiply(b,f))\n",
    "h = tf.add(tf.multiply(tf.add(b, c), d), tf.multiply(tf.pow(e, 2), c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#0b486b\"> I.4 More about Graph </span> ###\n",
    "Each TF project is associated with a default computational graph. Besides the default graph, we can create other graphs on demand and work with these graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "g1= tf.get_default_graph()\n",
    "g2 = tf.Graph()\n",
    "print(g1 is tf.get_default_graph())\n",
    "with g2.as_default():\n",
    "    print(g1 is tf.get_default_graph())\n",
    "    print(g2 is tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#0b486b\"> I.5 Data Types and Cast </span> ###\n",
    "TensorFlow supports several data types when declaring its nodes. We can cast a node from a data type to another. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant(name=\"x\", value=[1,2,3], dtype= tf.int32)\n",
    "print(\"x type is {}\".format(x.dtype))\n",
    "y = tf.cast(x, tf.float32)\n",
    "print(\"y type is {}\".format(y.dtype))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the popular data types in TensorFlow.\n",
    "\n",
    "<img src=\"images/TF_DataTypes.png\" width=\"600\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#0b486b\">I.6 Get Shape </span> ###\n",
    "Each node in a computational graph of TensorFlow is a tensor. We can use the method `get_shape()` to get shape of a tensor. Note that we create an interactive session and use `x.eval()` to evaluate the value of node `x` in the computational graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x= [[[ 1  2  3]\n",
      "  [ 4  5  6]]\n",
      "\n",
      " [[-1 -2 -3]\n",
      "  [-4 -5 -6]]]\n",
      "x shape: (2, 2, 3)\n"
     ]
    }
   ],
   "source": [
    "x= tf.constant([[[1,2,3],[4,5,6]],\n",
    "                [[-1,-2,-3],[-4,-5,-6]]])\n",
    "sess = tf.InteractiveSession()\n",
    "print(\"x= {}\".format(sess.run(x))) \n",
    "# Eval and sess.run() is the same, but remember to chuck everything into the run\n",
    "print(\"x shape: {}\".format(x.get_shape()))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#0b486b\"> I.7 Initialize Tensors </span> ###\n",
    "When declaring nodes in TensorFlow, we can initialize them using a heap of methods offered by TensorFlow. After that we can querry their values in an `interactive session`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "a= tf.constant(name='a', value=3)\n",
    "b= tf.fill(name='b', dims= [2,3], value=-1)\n",
    "c= tf.zeros(name='c', shape= [2,3])\n",
    "d= tf.ones(name='d', shape=[2,3])\n",
    "e= tf.random_normal(name='e', shape=[2,3], mean=0, stddev=1)\n",
    "f= tf.ones_like(e)\n",
    "g= tf.zeros_like(e)\n",
    "h= tf.random_shuffle([5,10,15,20])\n",
    "k= tf.random_uniform([2,3], minval=-1, maxval=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=3\n",
      "b=[[-1 -1 -1]\n",
      " [-1 -1 -1]]\n",
      "c=[[0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "d=[[1. 1. 1.]\n",
      " [1. 1. 1.]]\n",
      "e=[[ 1.1932441   0.10100866 -0.1715935 ]\n",
      " [ 0.24681237  1.1333975  -2.6883318 ]]\n",
      "f=[[1. 1. 1.]\n",
      " [1. 1. 1.]]\n",
      "g=[[0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "h=[15 20  5 10]\n",
      "k=[[-0.6018903   0.82583356  0.8424437 ]\n",
      " [-0.2821567  -0.94123673  0.5967517 ]]\n"
     ]
    }
   ],
   "source": [
    "sess= tf.InteractiveSession()\n",
    "print(\"a={}\".format(a.eval()))\n",
    "print(\"b={}\".format(b.eval()))\n",
    "print(\"c={}\".format(c.eval()))\n",
    "print(\"d={}\".format(d.eval()))\n",
    "print(\"e={}\".format(e.eval()))\n",
    "print(\"f={}\".format(f.eval()))\n",
    "print(\"g={}\".format(g.eval()))\n",
    "print(\"h={}\".format(h.eval()))\n",
    "print(\"k={}\".format(k.eval()))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#0b486b\"> I.8 Matrix Multiplication and Activation </span> ###\n",
    "Matrix multiplication in conjunction with activation is a building block in deep learning. In the following code snippet, we declare the matrix $A$ of $(2,3)$ and a vector $x$ of $(3,)$. To be able to apply $A \\times x$, we need to expand $x$ one dimension via the method `tf.expand_dims()`. Now $x$ has shape $(3,1)$ and we can do a matrix multiplication. Finnaly, we apply the activation function ReLu over the output of matrix multiplication: $ReLu(A \\times x)$. In addition, the formulation of ReLu is $ReLu(t)= \\max\\{0,t\\}$, which is the most popular activation function in deep learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A shape: (2, 3)\n",
      "x shape: (3,)\n",
      "x shape: (3, 1)\n",
      "y before activation: [[  6]\n",
      " [-15]]\n",
      "y after activation: [[6]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "A = tf.constant([[1,2,3],\n",
    "                 [-4,-5,-6]])\n",
    "x= tf.constant([1,1,1])\n",
    "print(\"A shape: {}\".format(A.get_shape()))\n",
    "print(\"x shape: {}\".format(x.get_shape()))\n",
    "x= tf.expand_dims(x,1) # expand one dimension to turn x's shape into (3, 1) \n",
    "print(\"x shape: {}\".format(x.get_shape()))\n",
    "sess= tf.InteractiveSession()\n",
    "y =tf.matmul(A,x)\n",
    "print(\"y before activation: {}\".format(y.eval()))\n",
    "y= tf.nn.relu(y) \n",
    "print(\"y after activation: {}\".format(y.eval()))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x= tf.expand_dims(x,1)\n",
    "print(\"x shape: {}\".format(x.get_shape()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color:red\">Exercise 3</span>**: Create a tensor $x$ of shape $(3, 1)$ with entry values $x[i, 0]=i$, $\\forall i = 0, 1, 2$ and another tensor $y$ of shape $(1, 3)$ with all entry values of $2$. Create another operation $z$ to perform matrix multiplication of $x$ and $y$. This operation is also called outer product. Print the shape of $z$. Also, run a querry to compute $z$ and print the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 1)\n",
      "(1, 3)\n",
      "[[0 0 0]\n",
      " [2 2 2]\n",
      " [4 4 4]]\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "sess = tf.InteractiveSession() \n",
    "x = tf.constant([[0], [1], [2]])\n",
    "y = tf.constant([[2,2,2]])\n",
    "print(x.get_shape())\n",
    "print(y.get_shape())\n",
    "z = tf.matmul(x, y)\n",
    "print(z.eval())\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#0b486b\">I.9 Name </span> ###\n",
    "Each tensor object also has an identifying name. This name is an *intrinsic* string name. We can use the `object.name` attribute to see the name of the object.\n",
    "Objects residing within the same graph cannot have the same name.\n",
    "\n",
    "\n",
    "TF will automatically add an underscore and a number to distinguish the two which accidentally have the same name. Both objects can have the same name when they are associated with different graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c1 name: c:0\n",
      "c2 name: c_1:0\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "g = tf.get_default_graph()\n",
    "with g.as_default():\n",
    "    c1= tf.constant(name='c', value=3)\n",
    "    c2= tf.constant(name='c', value=5)\n",
    "print(\"c1 name: {}\".format(c1.name))\n",
    "print(\"c2 name: {}\".format(c2.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#0b486b\">I.10 Name Scope </span> ### \n",
    "Name scope is a *hierarchically group nodes* together by name to divide a graph into subgraphs with some semantic meaning. To declare a name space, we use the syntax: `tf.name_scope(\"prefix\")`.\n",
    "\n",
    "Organizing your TensorFlow code using name scopes shows some advantages:\n",
    "- Make it easier to follow and manage.\n",
    "- Visualization of the graph structure.\n",
    "- Useful when dealing with a large, complicated graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c1 name: c_2:0\n",
      "c2 name: prefix/c:0\n",
      "c3 name: prefix/c_1:0\n"
     ]
    }
   ],
   "source": [
    "with tf.get_default_graph().as_default():\n",
    "    c1 = tf.constant(name= \"c\", value=1.0)\n",
    "    with tf.name_scope(\"prefix\"):\n",
    "        c2 = tf.constant(name= \"c\", value=2.0)\n",
    "        c3 = tf.constant(name= \"c\", value=3.0)\n",
    "print(\"c1 name: {}\".format(c1.name))\n",
    "print(\"c2 name: {}\".format(c2.name))\n",
    "print(\"c3 name: {}\".format(c3.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#0b486b\">I.11 Variable </span> ### \n",
    "Variable nodes are crucial nodes in a TensorFlow computational graph whose values can be *modified* and *changed*. \n",
    "\n",
    "In a deep learning model, `model parameters` are declared as `variable nodes` in the relevant computational graph whose values are modified during training course. Using variables is done in two stages:\n",
    "- Call the `tf.Variable()` function to create a variable and define what value it will be initialized with.\n",
    "- Explicitly perform an initialization operation by running the session with the `tf.global_variables_initializer()` method. This will allocate the memory for the variable and set its initial values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-run my_var: <tf.Variable 'x:0' shape=(2, 3) dtype=float32_ref>\n",
      "Post-run my_var: [[-0.09475895 -0.11482976  0.00464236]\n",
      " [-0.07952299 -0.05104303 -0.10303134]]\n"
     ]
    }
   ],
   "source": [
    "# Remember the initial value are random values for Dl, then we update based on Baclpropagation\n",
    "init_var = tf.random.normal(shape=[2,3], mean=0, stddev=0.1, dtype= tf.float32) \n",
    "my_var=  tf.Variable(initial_value= init_var, name='x')\n",
    "init= tf.global_variables_initializer()\n",
    "print(\"Pre-run my_var: {}\".format(my_var))\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    my_var = sess.run(my_var)\n",
    "    print(\"Post-run my_var: {}\".format(my_var))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below presents some popular ways to *randomly initialize* the intial values of variables.\n",
    "\n",
    "<img src=\"images/tf_random.png\" width=\"800\" align=\"center\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#0b486b\">I.12 Variable Scope and Reuse Variable </span> ###  \n",
    "\n",
    "Sometimes we might want to reuse a variable. This can be done by `tf.get_variable()`, which either reuses the variable with a specified name or creates a new variable with the name if it is not created before.\n",
    "\n",
    "If we want to reuse a variable later, we first need to declare this in a variable scope with `tf.variable_scope()` and then in this variable scope invoke `tf.get_variable()` with the flag reuse to be set to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Variable layer1/W already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"d:\\Work Stuff\\Monash Stuff\\Year 3 Semester 1\\FIT3181 Deep Learning\\FIT3181-Deep-Learning\\Week 1\\wk1\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2133, in __init__\n    self._traceback = tf_stack.extract_stack_for_node(self._c_op)\n  File \"d:\\Work Stuff\\Monash Stuff\\Year 3 Semester 1\\FIT3181 Deep Learning\\FIT3181-Deep-Learning\\Week 1\\wk1\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3754, in _create_op_internal\n    ret = Operation(\n  File \"d:\\Work Stuff\\Monash Stuff\\Year 3 Semester 1\\FIT3181 Deep Learning\\FIT3181-Deep-Learning\\Week 1\\wk1\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 797, in _apply_op_helper\n    op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n  File \"d:\\Work Stuff\\Monash Stuff\\Year 3 Semester 1\\FIT3181 Deep Learning\\FIT3181-Deep-Learning\\Week 1\\wk1\\lib\\site-packages\\tensorflow\\python\\ops\\gen_state_ops.py\", line 1750, in variable_v2\n    _, _, _op, _outputs = _op_def_library._apply_op_helper(\n  File \"d:\\Work Stuff\\Monash Stuff\\Year 3 Semester 1\\FIT3181 Deep Learning\\FIT3181-Deep-Learning\\Week 1\\wk1\\lib\\site-packages\\tensorflow\\python\\ops\\state_ops.py\", line 70, in variable_op_v2\n    return gen_state_ops.variable_v2(\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\Work Stuff\\Monash Stuff\\Year 3 Semester 1\\FIT3181 Deep Learning\\FIT3181-Deep-Learning\\Week 1\\FIT3181 Deep Learning Tutorial\\FIT3181_Tut_01_Tour_TF.ipynb Cell 37\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Work%20Stuff/Monash%20Stuff/Year%203%20Semester%201/FIT3181%20Deep%20Learning/FIT3181-Deep-Learning/Week%201/FIT3181%20Deep%20Learning%20Tutorial/FIT3181_Tut_01_Tour_TF.ipynb#ch0000036?line=0'>1</a>\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mvariable_scope(\u001b[39m\"\u001b[39m\u001b[39mlayer1\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Work%20Stuff/Monash%20Stuff/Year%203%20Semester%201/FIT3181%20Deep%20Learning/FIT3181-Deep-Learning/Week%201/FIT3181%20Deep%20Learning%20Tutorial/FIT3181_Tut_01_Tour_TF.ipynb#ch0000036?line=1'>2</a>\u001b[0m     W1\u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mget_variable(name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mW\u001b[39;49m\u001b[39m'\u001b[39;49m, shape\u001b[39m=\u001b[39;49m[\u001b[39m2\u001b[39;49m,\u001b[39m3\u001b[39;49m], dtype\u001b[39m=\u001b[39;49m tf\u001b[39m.\u001b[39;49mfloat32, initializer\u001b[39m=\u001b[39;49m tf\u001b[39m.\u001b[39;49minitializers\u001b[39m.\u001b[39;49mrandom_normal(\u001b[39m0\u001b[39;49m,\u001b[39m0.1\u001b[39;49m))\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Work%20Stuff/Monash%20Stuff/Year%203%20Semester%201/FIT3181%20Deep%20Learning/FIT3181-Deep-Learning/Week%201/FIT3181%20Deep%20Learning%20Tutorial/FIT3181_Tut_01_Tour_TF.ipynb#ch0000036?line=2'>3</a>\u001b[0m     x\u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mVariable(initial_value\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mrandom_normal([\u001b[39m4\u001b[39m,\u001b[39m2\u001b[39m], \u001b[39m0\u001b[39m, \u001b[39m0.1\u001b[39m), name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mx\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Work%20Stuff/Monash%20Stuff/Year%203%20Semester%201/FIT3181%20Deep%20Learning/FIT3181-Deep-Learning/Week%201/FIT3181%20Deep%20Learning%20Tutorial/FIT3181_Tut_01_Tour_TF.ipynb#ch0000036?line=3'>4</a>\u001b[0m     v1\u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mmatmul(x, W1)\n",
      "File \u001b[1;32md:\\Work Stuff\\Monash Stuff\\Year 3 Semester 1\\FIT3181 Deep Learning\\FIT3181-Deep-Learning\\Week 1\\wk1\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py:1616\u001b[0m, in \u001b[0;36mget_variable\u001b[1;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m   1600\u001b[0m \u001b[39m@tf_export\u001b[39m(v1\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mget_variable\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m   1601\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_variable\u001b[39m(name,\n\u001b[0;32m   1602\u001b[0m                  shape\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1614\u001b[0m                  synchronization\u001b[39m=\u001b[39mVariableSynchronization\u001b[39m.\u001b[39mAUTO,\n\u001b[0;32m   1615\u001b[0m                  aggregation\u001b[39m=\u001b[39mVariableAggregation\u001b[39m.\u001b[39mNONE):\n\u001b[1;32m-> 1616\u001b[0m   \u001b[39mreturn\u001b[39;00m get_variable_scope()\u001b[39m.\u001b[39;49mget_variable(\n\u001b[0;32m   1617\u001b[0m       _get_default_variable_store(),\n\u001b[0;32m   1618\u001b[0m       name,\n\u001b[0;32m   1619\u001b[0m       shape\u001b[39m=\u001b[39;49mshape,\n\u001b[0;32m   1620\u001b[0m       dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m   1621\u001b[0m       initializer\u001b[39m=\u001b[39;49minitializer,\n\u001b[0;32m   1622\u001b[0m       regularizer\u001b[39m=\u001b[39;49mregularizer,\n\u001b[0;32m   1623\u001b[0m       trainable\u001b[39m=\u001b[39;49mtrainable,\n\u001b[0;32m   1624\u001b[0m       collections\u001b[39m=\u001b[39;49mcollections,\n\u001b[0;32m   1625\u001b[0m       caching_device\u001b[39m=\u001b[39;49mcaching_device,\n\u001b[0;32m   1626\u001b[0m       partitioner\u001b[39m=\u001b[39;49mpartitioner,\n\u001b[0;32m   1627\u001b[0m       validate_shape\u001b[39m=\u001b[39;49mvalidate_shape,\n\u001b[0;32m   1628\u001b[0m       use_resource\u001b[39m=\u001b[39;49muse_resource,\n\u001b[0;32m   1629\u001b[0m       custom_getter\u001b[39m=\u001b[39;49mcustom_getter,\n\u001b[0;32m   1630\u001b[0m       constraint\u001b[39m=\u001b[39;49mconstraint,\n\u001b[0;32m   1631\u001b[0m       synchronization\u001b[39m=\u001b[39;49msynchronization,\n\u001b[0;32m   1632\u001b[0m       aggregation\u001b[39m=\u001b[39;49maggregation)\n",
      "File \u001b[1;32md:\\Work Stuff\\Monash Stuff\\Year 3 Semester 1\\FIT3181 Deep Learning\\FIT3181-Deep-Learning\\Week 1\\wk1\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py:1326\u001b[0m, in \u001b[0;36mVariableScope.get_variable\u001b[1;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m   1324\u001b[0m \u001b[39mif\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1325\u001b[0m   dtype \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dtype\n\u001b[1;32m-> 1326\u001b[0m \u001b[39mreturn\u001b[39;00m var_store\u001b[39m.\u001b[39;49mget_variable(\n\u001b[0;32m   1327\u001b[0m     full_name,\n\u001b[0;32m   1328\u001b[0m     shape\u001b[39m=\u001b[39;49mshape,\n\u001b[0;32m   1329\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m   1330\u001b[0m     initializer\u001b[39m=\u001b[39;49minitializer,\n\u001b[0;32m   1331\u001b[0m     regularizer\u001b[39m=\u001b[39;49mregularizer,\n\u001b[0;32m   1332\u001b[0m     reuse\u001b[39m=\u001b[39;49mreuse,\n\u001b[0;32m   1333\u001b[0m     trainable\u001b[39m=\u001b[39;49mtrainable,\n\u001b[0;32m   1334\u001b[0m     collections\u001b[39m=\u001b[39;49mcollections,\n\u001b[0;32m   1335\u001b[0m     caching_device\u001b[39m=\u001b[39;49mcaching_device,\n\u001b[0;32m   1336\u001b[0m     partitioner\u001b[39m=\u001b[39;49mpartitioner,\n\u001b[0;32m   1337\u001b[0m     validate_shape\u001b[39m=\u001b[39;49mvalidate_shape,\n\u001b[0;32m   1338\u001b[0m     use_resource\u001b[39m=\u001b[39;49muse_resource,\n\u001b[0;32m   1339\u001b[0m     custom_getter\u001b[39m=\u001b[39;49mcustom_getter,\n\u001b[0;32m   1340\u001b[0m     constraint\u001b[39m=\u001b[39;49mconstraint,\n\u001b[0;32m   1341\u001b[0m     synchronization\u001b[39m=\u001b[39;49msynchronization,\n\u001b[0;32m   1342\u001b[0m     aggregation\u001b[39m=\u001b[39;49maggregation)\n",
      "File \u001b[1;32md:\\Work Stuff\\Monash Stuff\\Year 3 Semester 1\\FIT3181 Deep Learning\\FIT3181-Deep-Learning\\Week 1\\wk1\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py:582\u001b[0m, in \u001b[0;36m_VariableStore.get_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m    580\u001b[0m   \u001b[39mreturn\u001b[39;00m custom_getter(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcustom_getter_kwargs)\n\u001b[0;32m    581\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 582\u001b[0m   \u001b[39mreturn\u001b[39;00m _true_getter(\n\u001b[0;32m    583\u001b[0m       name,\n\u001b[0;32m    584\u001b[0m       shape\u001b[39m=\u001b[39;49mshape,\n\u001b[0;32m    585\u001b[0m       dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m    586\u001b[0m       initializer\u001b[39m=\u001b[39;49minitializer,\n\u001b[0;32m    587\u001b[0m       regularizer\u001b[39m=\u001b[39;49mregularizer,\n\u001b[0;32m    588\u001b[0m       reuse\u001b[39m=\u001b[39;49mreuse,\n\u001b[0;32m    589\u001b[0m       trainable\u001b[39m=\u001b[39;49mtrainable,\n\u001b[0;32m    590\u001b[0m       collections\u001b[39m=\u001b[39;49mcollections,\n\u001b[0;32m    591\u001b[0m       caching_device\u001b[39m=\u001b[39;49mcaching_device,\n\u001b[0;32m    592\u001b[0m       partitioner\u001b[39m=\u001b[39;49mpartitioner,\n\u001b[0;32m    593\u001b[0m       validate_shape\u001b[39m=\u001b[39;49mvalidate_shape,\n\u001b[0;32m    594\u001b[0m       use_resource\u001b[39m=\u001b[39;49muse_resource,\n\u001b[0;32m    595\u001b[0m       constraint\u001b[39m=\u001b[39;49mconstraint,\n\u001b[0;32m    596\u001b[0m       synchronization\u001b[39m=\u001b[39;49msynchronization,\n\u001b[0;32m    597\u001b[0m       aggregation\u001b[39m=\u001b[39;49maggregation)\n",
      "File \u001b[1;32md:\\Work Stuff\\Monash Stuff\\Year 3 Semester 1\\FIT3181 Deep Learning\\FIT3181-Deep-Learning\\Week 1\\wk1\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py:535\u001b[0m, in \u001b[0;36m_VariableStore.get_variable.<locals>._true_getter\u001b[1;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m    529\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m/part_0\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_vars:\n\u001b[0;32m    530\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    531\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mNo partitioner was provided, but a partitioned version of the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    532\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mvariable was found: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m/part_0. Perhaps a variable of the same \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    533\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mname was already created with partitioning?\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m name)\n\u001b[1;32m--> 535\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_single_variable(\n\u001b[0;32m    536\u001b[0m     name\u001b[39m=\u001b[39;49mname,\n\u001b[0;32m    537\u001b[0m     shape\u001b[39m=\u001b[39;49mshape,\n\u001b[0;32m    538\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m    539\u001b[0m     initializer\u001b[39m=\u001b[39;49minitializer,\n\u001b[0;32m    540\u001b[0m     regularizer\u001b[39m=\u001b[39;49mregularizer,\n\u001b[0;32m    541\u001b[0m     reuse\u001b[39m=\u001b[39;49mreuse,\n\u001b[0;32m    542\u001b[0m     trainable\u001b[39m=\u001b[39;49mtrainable,\n\u001b[0;32m    543\u001b[0m     collections\u001b[39m=\u001b[39;49mcollections,\n\u001b[0;32m    544\u001b[0m     caching_device\u001b[39m=\u001b[39;49mcaching_device,\n\u001b[0;32m    545\u001b[0m     validate_shape\u001b[39m=\u001b[39;49mvalidate_shape,\n\u001b[0;32m    546\u001b[0m     use_resource\u001b[39m=\u001b[39;49muse_resource,\n\u001b[0;32m    547\u001b[0m     constraint\u001b[39m=\u001b[39;49mconstraint,\n\u001b[0;32m    548\u001b[0m     synchronization\u001b[39m=\u001b[39;49msynchronization,\n\u001b[0;32m    549\u001b[0m     aggregation\u001b[39m=\u001b[39;49maggregation)\n",
      "File \u001b[1;32md:\\Work Stuff\\Monash Stuff\\Year 3 Semester 1\\FIT3181 Deep Learning\\FIT3181-Deep-Learning\\Week 1\\wk1\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py:898\u001b[0m, in \u001b[0;36m_VariableStore._get_single_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[0;32m    893\u001b[0m   \u001b[39m# Throw away internal tf entries and only take a few lines. In some\u001b[39;00m\n\u001b[0;32m    894\u001b[0m   \u001b[39m# cases the traceback can be longer (e.g. if someone uses factory\u001b[39;00m\n\u001b[0;32m    895\u001b[0m   \u001b[39m# functions to create variables) so we take more than needed in the\u001b[39;00m\n\u001b[0;32m    896\u001b[0m   \u001b[39m# default case.\u001b[39;00m\n\u001b[0;32m    897\u001b[0m   tb \u001b[39m=\u001b[39m [x \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m tb \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mtensorflow/python\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m x[\u001b[39m0\u001b[39m]][:\u001b[39m5\u001b[39m]\n\u001b[1;32m--> 898\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m Originally defined at:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m    899\u001b[0m                    (err_msg, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(traceback\u001b[39m.\u001b[39mformat_list(tb))))\n\u001b[0;32m    900\u001b[0m found_var \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_vars[name]\n\u001b[0;32m    901\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m shape\u001b[39m.\u001b[39mis_compatible_with(found_var\u001b[39m.\u001b[39mget_shape()):\n",
      "\u001b[1;31mValueError\u001b[0m: Variable layer1/W already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"d:\\Work Stuff\\Monash Stuff\\Year 3 Semester 1\\FIT3181 Deep Learning\\FIT3181-Deep-Learning\\Week 1\\wk1\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2133, in __init__\n    self._traceback = tf_stack.extract_stack_for_node(self._c_op)\n  File \"d:\\Work Stuff\\Monash Stuff\\Year 3 Semester 1\\FIT3181 Deep Learning\\FIT3181-Deep-Learning\\Week 1\\wk1\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3754, in _create_op_internal\n    ret = Operation(\n  File \"d:\\Work Stuff\\Monash Stuff\\Year 3 Semester 1\\FIT3181 Deep Learning\\FIT3181-Deep-Learning\\Week 1\\wk1\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 797, in _apply_op_helper\n    op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n  File \"d:\\Work Stuff\\Monash Stuff\\Year 3 Semester 1\\FIT3181 Deep Learning\\FIT3181-Deep-Learning\\Week 1\\wk1\\lib\\site-packages\\tensorflow\\python\\ops\\gen_state_ops.py\", line 1750, in variable_v2\n    _, _, _op, _outputs = _op_def_library._apply_op_helper(\n  File \"d:\\Work Stuff\\Monash Stuff\\Year 3 Semester 1\\FIT3181 Deep Learning\\FIT3181-Deep-Learning\\Week 1\\wk1\\lib\\site-packages\\tensorflow\\python\\ops\\state_ops.py\", line 70, in variable_op_v2\n    return gen_state_ops.variable_v2(\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope(\"layer1\"):\n",
    "    W1= tf.get_variable(name='W', shape=[2,3], dtype= tf.float32, initializer= tf.initializers.random_normal(0,0.1))\n",
    "    x= tf.Variable(initial_value=tf.random_normal([4,2], 0, 0.1), name=\"x\")\n",
    "    v1= tf.matmul(x, W1)\n",
    "\n",
    "with tf.variable_scope(\"layer1\", reuse=True):\n",
    "    W2= tf.get_variable(name='W', shape=[2,3], dtype= tf.float32, initializer= tf.initializers.random_normal(0,0.2))\n",
    "\n",
    "diff = tf.subtract(W1,W2)\n",
    "norm_diff = tf.norm(diff, ord='euclidean')\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    norm_diff= sess.run(norm_diff)\n",
    "\n",
    "if norm_diff==0:\n",
    "    print(\"W1 and W2 are tight\")\n",
    "else:\n",
    "    print(\"W1 and W2 are not tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#0b486b\">I.13 Name Scope & Variable Scope </span> ###   \n",
    "There are two different types of scopes: `name scope` created using `tf.name_scope` and `variable scope` created using `tf.variable_scope`. Both scopes have the same effect on all operations as well as variables created using `tf.Variable`, while only variable scope affects on `tf.get_variable()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following figure shows the difference of two ways to create variables: `tf.Variable` and `tf.get_variable()`. <br/>\n",
    "\n",
    "<img src=\"images/Compare2WaysVariables.png\" width=\"400\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c1 name: ns/vs/c:0\n",
      "c2 name: vs/c:0\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.name_scope(\"ns\"):\n",
    "    with tf.variable_scope(\"vs\", reuse=tf.AUTO_REUSE):\n",
    "        c1= tf.Variable(name=\"c\", initial_value= tf.constant(1.0))\n",
    "        c2= tf.get_variable(name=\"c\", initializer= tf.constant([-1,1]))\n",
    "print(\"c1 name: {}\".format(c1.name))\n",
    "print(\"c2 name: {}\".format(c2.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color:red\">Exercise 4</span>**: TensorFlow can also automatically decide to create a new variable if it does not exist and reuse if it is already created by setting `reuse=tf.AUTO_REUSE` as shown in the function `linear()` below. Questions:\n",
    "\n",
    "(a) What is the value of `diff`? You can run a session to compute its value to answer this question.\n",
    "\n",
    "(b) Why does `diff` get that value?\n",
    "\n",
    "(c) What happens if we set `reuse=False` in the delaration of `z` in the code below? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "def linear(x, output_dim, reuse=tf.AUTO_REUSE):\n",
    "    with tf.variable_scope(\"linear\", reuse=reuse):\n",
    "        W = tf.get_variable('weights', shape=[output_dim, x.get_shape()[0]], dtype=tf.float32)\n",
    "        output = tf.matmul(W, x)\n",
    "        return output\n",
    "    \n",
    "x = tf.constant(0.0, shape=(3, 1))\n",
    "y = linear(x, 3, reuse=tf.AUTO_REUSE) \n",
    "z = linear(x, 3, reuse=tf.AUTO_REUSE) \n",
    "diff = y - z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(diff.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write your answer here:**\n",
    "\n",
    "(a)\n",
    "\n",
    "(b)\n",
    "\n",
    "(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#0b486b\">I.14 Place Holder </span> ###    \n",
    "Placeholders can be thought of as `empty variables` that will be filled with data later on.\n",
    "\n",
    "We use them when constructing our graph and when executing querying value of nodes, we need to feed them with the input data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s= -0.14842142164707184\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "tf.reset_default_graph()\n",
    "x_data= np.random.rand(10,3)\n",
    "\n",
    "with tf.name_scope(\"layer1\"):\n",
    "    W = tf.Variable(name= \"W\", initial_value=tf.random.normal([1,10],0,0.1, dtype= tf.float32))\n",
    "    b = tf.Variable(name=\"b\", initial_value=tf.random.normal([1,3],0,0.1, dtype=tf.float32))\n",
    "    x = tf.placeholder(name=\"x\", shape=[10,3], dtype= tf.float32)\n",
    "    v = tf.matmul(W,x) +b\n",
    "    s = tf.reduce_mean(v)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    s= sess.run(s, feed_dict={x:x_data}) # Feed in the placeholder value here using feed_dict here\n",
    "\n",
    "print(\"s= {}\".format(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#0b486b\">I.15. Visualization with TensorBoard</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <span style=\"color:#0b486b\"> Logging and creating summary for visualization </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually one would use the `print()` function and `matplotlib` to visualize progress during training. There is a better way to do this via TensorBoard. If you feed in training stats, it will display nice interactive visualizations of\n",
    "these stats in your web browser (e.g., learning curves).\n",
    "\n",
    "You can also provide the graph’s definition and TensorBoard provides interface to browse through it. This is very useful to identify errors in the graph, to find bottlenecks, and so on.\n",
    "\n",
    "Let's visualize learning rate and global step in the example above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construction\n",
    "tf.reset_default_graph()\n",
    "\n",
    "starter_lr = 1.\n",
    "decay_rate = 0.9\n",
    "global_step = tf.Variable(0., trainable=0)\n",
    "incr = tf.assign(global_step, global_step + 1)\n",
    "\n",
    "with tf.control_dependencies([incr]):\n",
    "    learning_rate = starter_lr * tf.pow(decay_rate, global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we construct the graph as normal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.summary.scalar('learning_rate', learning_rate)\n",
    "tf.summary.scalar('global_step', global_step)\n",
    "merged = tf.summary.merge_all() # Merges all summaries collected in the default graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first two lines create two summary ops in the graph that will evaluate the learning_rate and global_step value and write them to a TensorBoard compatible binary log string called a summary.\n",
    "\n",
    "The third line creates a node that merges all summaries collected in the default graph. In the execution phase, you'll need to evaluate the merged node regularly during training (e.g., every 10 mini-batches). This will output a summary that you can then write to the events file using the *`file_writer`*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "logdir = \"tf_logs/example01/model-at-{}\".format(time.strftime('%Y-%m-%d_%H.%M.%S'))\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/note.gif\" width=\"40\" align=\"left\"> Tip: You need to use a different log directory every time you run your program, or else TensorBoard will merge stats from different runs, which will mess up the visualizations. The simplest solution for this is to include a timestamp in the log directory name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now's the execution phase: <br>\n",
    "\\- The first line creates a node in the graph that will evaluate the *MSE* value and write it to a TensorBoard compatible binary log string called a summary. Then you need to update the execution phase to evaluate the *`mse_summary`* node regularly during training\n",
    "(e.g., every 10 mini-batches). This will output a summary that you can then write to the events file using\n",
    "the *`file_writer`*. Here is the updated code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    global_step.initializer.run()\n",
    "    for i in range(50):\n",
    "        merged_ = merged.eval()\n",
    "        file_writer.add_summary(merged_, i+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/warning.png\" width=\"40\" align=\"left\"></img> **Warning**: *In real-world application logging training stats at every single training step would significantly slow down training; instead, one should log at regular interval, such as after each 200 iterations*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you want to close the FileWriter at the end of the program:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now it's time to show Tensorboard. Fortunately, we can run and show tensorboard within the notebook by running the command line as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-25b6e65a898ec59f\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-25b6e65a898ec59f\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir tf_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otherwise, you can open Tensorboard in a separate browser's window. You need to activate your virtual environment\n",
    "if you created one, then start the server by running the *`tensorboard`* command, pointing it to the root log\n",
    "directory. This starts the TensorBoard web server, listening on port 6006 <br> <br>\n",
    "- Open command line, nevigate to the folder of this tute and run **> tensorboard --logdir tf_logs**\n",
    "- Open your browser and go to https://localhost:6006. Welcome to\n",
    "TensorBoard! In the Scalars tab, you'll see *`global_step`* and *`learning_rate`*: <br><br>\n",
    "\n",
    "<img src='images/learning_rate.png' width=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#0b486b\"> II. Understanding TensorFlow 2.x </span> <span style=\"color:red\">*****</span>\n",
    "\n",
    "### Table of Content \n",
    "II.1 Advancements of TensorFlow 2. <br>\n",
    "II.2 TensorFlow 2.x's API and architecture. <br>\n",
    "II.3 Eager Execution and Dynamic Graph. <br> \n",
    "II.4 Placeholder Replacement and The Data API in TF 2.x <br> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#0b486b\"> II.1. Advancements of TensorFlow 2.x </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In TF 1.x, we need to declare a computational graph manually and create session to query node values. This is different from an imperative and high-level programing language such as Python, which is much easier to work with and more dynamic. TF 2.x brings TF to closer to an imperative and high-level programming language such as Python, thereby making the task of building up deep learning models more conveniently. This is due to two new features: **eager execution** and **AutoGraph**.\n",
    "- **Eager execution**: you still have a graph, but you can define, change, and execute nodes on-the-ﬂy, with no special session interfaces or placeholders. This is what is called eager execution, meaning that the model definitions are dynamic, and the execution is immediate. Graphs and sessions should be considered as implementation details. In TF 2.x, we do not need to work directly with a computational graph and session.\n",
    "- **AutoGraph**: AutoGraph takes eager-style Python code and automatically converts it to graph-generating code. So, again, transparently TensorFlow 2.x creates a bridge between imperative, dynamic, and eager Python style programming with efficient graph computations, taking the best of both worlds.\n",
    "\n",
    "In addition, Keras has been incorporated in and become a part of TF 2.x, which allows us to build up deep learning models comfortably and conveniently.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#0b486b\"> II.2. TensorFlow 2.x's API and architecture</span>\n",
    "\n",
    "The below image shows the API of TF 2.x. At the lowest level, each TensorFlow operation (op for short) is implemented using highly efficient C++ code. Many operations have multiple implementations called kernels: each kernel is dedicated to a specific device type, such as CPUs, GPUs, or even TPUs (tensor processing units).\n",
    "\n",
    "<img src='images/TF_API.png' align=center width=600>\n",
    "\n",
    "\n",
    "The architecture of TF 2.x is shown in the following image. Most of the time your code will use the high-level APIs (especially tf.keras and tf.data); but when you need more flexibility, you will use the lower-level Python API, handling tensors directly. \n",
    "\n",
    "<img src='images/TF_Architecture.png' align=center width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#0b486b\"> II.3. Eager Execution in TF 2.x</span>\n",
    "\n",
    "#### Define, Change and Execute on-the-fly \n",
    "For example, you can define a variable, then modify it and showing the current value of this variable without calling any session or global_variables_initializer function as in TF 1.x  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "\n",
    "v = tf.Variable(tf.ones([2,3], dtype= tf.float32)) \n",
    "print('--------')\n",
    "print('Define a variable which has initial value is a tensor one with shape [2,3]')\n",
    "print(v)\n",
    "\n",
    " \n",
    "v.assign_add(10. * tf.ones_like(v))\n",
    "print('--------')\n",
    "print('In place increasing by 10 ')\n",
    "print(v)\n",
    "\n",
    " \n",
    "v[0,0].assign(-1)\n",
    "print('--------')\n",
    "print('Modify individual cells in the tensor, e.g., cell [0,0] to -1 ')\n",
    "print(v)\n",
    "\n",
    "\n",
    "# Modify a specific cell inside a forloop \n",
    "print('--------')\n",
    "for i in range(v.shape[1]): \n",
    "#     v[:,i] = i # Raise Error: 'ResourceVariable' object does not support item assignment \n",
    "#     v[:,i].assign(i) # does not match r-value shape []. Automatic broadcasting not yet implemented \n",
    "    v[0,i].assign(i) # Working \n",
    "    print('-- iter = {} --'.format(i))\n",
    "    print(v)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variable Scope and Reuse Variable in TF 2.x \n",
    "In TF 1.x you need to manage the variable scope of each variable, especially when reusing it. In TF 2.x, it is more flexible.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example in TF 1.x \n",
    "\n",
    "#import tensorflow as tf \n",
    "#tf.compat.v1.disable_v2_behavior()\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "# Scenario 1: an input x that shared with two weights W1 and W2 \n",
    "print('-----------------------------')\n",
    "x = tf.Variable(tf.random.normal(shape=[4,2], mean=0., stddev=0.1))\n",
    "W1 = tf.Variable(tf.random.normal(shape=[2,3], mean=0., stddev=0.1))\n",
    "W2 = tf.Variable(tf.random.normal(shape=[2,5], mean=0., stddev=0.1))\n",
    "v1 = tf.matmul(x, W1)\n",
    "v2 = tf.matmul(x, W2)\n",
    "\n",
    "print('v1', v1)\n",
    "print('v2', v2)\n",
    "\n",
    "# Scenario 2: two input x1 and x2 multiple with the same weight W \n",
    "print('-----------------------------')\n",
    "x1 = tf.Variable(tf.random.normal(shape=[4,2], mean=0., stddev=0.1))\n",
    "x2 = tf.Variable(tf.random.normal(shape=[1,2], mean=0., stddev=0.1))\n",
    "W = tf.Variable(tf.random.normal(shape=[2,3], mean=0., stddev=0.1))\n",
    "v1 = tf.matmul(x1, W)\n",
    "v2 = tf.matmul(x2, W)\n",
    "\n",
    "print('v1', v1)\n",
    "print('v2', v2)\n",
    "\n",
    "# Scenario 3: Define a function that is matrix multiplication with pre-defined weight W \n",
    "print('-----------------------------')\n",
    "W = tf.Variable(tf.random.normal(shape=[2,3], mean=0., stddev=0.1))\n",
    "def my_matmul(x): \n",
    "    return tf.matmul(x, W)\n",
    "\n",
    "x1 = tf.Variable(tf.random.normal(shape=[4,2], mean=0., stddev=0.1))\n",
    "x2 = tf.Variable(tf.random.normal(shape=[1,2], mean=0., stddev=0.1))\n",
    "v1 = my_matmul(x1)\n",
    "v2 = my_matmul(x2)\n",
    "print('v1', v1)\n",
    "print('v2', v2)\n",
    "\n",
    "# Scenario 4: Define a function that is matrix multiplication with weight W is defined inside the function. \n",
    "# later in this unit, you will see some common practices, e.g., a convolution layer \n",
    "print('-----------------------------')\n",
    "def my_matmul2(x): \n",
    "    with tf.variable_scope(\"W\", reuse=tf.AUTO_REUSE):\n",
    "        W = tf.get_variable(name='W', shape=[2,3], dtype= tf.float32, initializer= tf.initializers.random_normal(0,0.1))\n",
    "    return tf.matmul(x, W)\n",
    "x1 = tf.Variable(tf.random.normal(shape=[4,2], mean=0., stddev=0.1))\n",
    "x2 = tf.Variable(tf.random.normal(shape=[1,2], mean=0., stddev=0.1))\n",
    "v1 = my_matmul2(x1)\n",
    "v2 = my_matmul2(x2)\n",
    "print('v1', v1)\n",
    "print('v2', v2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to restart kernel if enabling v1 behavior before ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproduce in TF 2.x \n",
    "# Need to restart kernel if enabling v1 behavior before \n",
    "\n",
    "#import tensorflow as tf \n",
    "#tf.compat.v1.enable_v2_behavior()\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.enable_v2_behavior()\n",
    "\n",
    "# Scenario 1: an input x that shared with two weights W1 and W2 \n",
    "print('-----------------------------')\n",
    "x = tf.Variable(tf.random.normal(shape=[4,2], mean=0., stddev=0.1))\n",
    "W1 = tf.Variable(tf.random.normal(shape=[2,3], mean=0., stddev=0.1))\n",
    "W2 = tf.Variable(tf.random.normal(shape=[2,5], mean=0., stddev=0.1))\n",
    "v1 = tf.matmul(x, W1)\n",
    "v2 = tf.matmul(x, W2)\n",
    "\n",
    "print('v1', v1)\n",
    "print('v2', v2)\n",
    "\n",
    "# Scenario 2: two input x1 and x2 multiple with the same weight W \n",
    "print('-----------------------------')\n",
    "x1 = tf.Variable(tf.random.normal(shape=[4,2], mean=0., stddev=0.1))\n",
    "x2 = tf.Variable(tf.random.normal(shape=[1,2], mean=0., stddev=0.1))\n",
    "W = tf.Variable(tf.random.normal(shape=[2,3], mean=0., stddev=0.1))\n",
    "v1 = tf.matmul(x1, W)\n",
    "v2 = tf.matmul(x2, W)\n",
    "\n",
    "print('v1', v1)\n",
    "print('v2', v2)\n",
    "\n",
    "# Scenario 3: Define a function that is matrix multiplication with pre-defined weight W \n",
    "print('-----------------------------')\n",
    "W = tf.Variable(tf.random.normal(shape=[2,3], mean=0., stddev=0.1))\n",
    "def my_matmul(x): \n",
    "    return tf.matmul(x, W)\n",
    "\n",
    "x1 = tf.Variable(tf.random.normal(shape=[4,2], mean=0., stddev=0.1))\n",
    "x2 = tf.Variable(tf.random.normal(shape=[1,2], mean=0., stddev=0.1))\n",
    "v1 = my_matmul(x1)\n",
    "v2 = my_matmul(x2)\n",
    "print('v1', v1)\n",
    "print('v2', v2)\n",
    "\n",
    "# Scenario 4: Define a function that is matrix multiplication with weight W is defined inside the function. \n",
    "# later in this unit, you will see some common practices, e.g., a convolution layer \n",
    "print('-----------------------------')\n",
    "def my_matmul2(x): \n",
    "    W = tf.Variable(tf.random.normal(shape=[2,3], mean=0., stddev=0.1))\n",
    "    return tf.matmul(x, W)\n",
    "\n",
    "x1 = tf.Variable(tf.random.normal(shape=[4,2], mean=0., stddev=0.1))\n",
    "x2 = tf.Variable(tf.random.normal(shape=[1,2], mean=0., stddev=0.1))\n",
    "v1 = my_matmul2(x1)\n",
    "print('v1', v1)\n",
    "print('W', W)\n",
    "\n",
    "v2 = my_matmul2(x2)\n",
    "\n",
    "print('v2', v2)\n",
    "print('W', W)\n",
    "print('Using the same W in two calls')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#0b486b\"> AutoGraph and Tracing</span>\n",
    "To fulfill the autograph feature, TF 2.x need to parse and analyze Python functions to capture all the control flow statements, such as *for loops, while loops*, and *if* statements, as well as *break, continue*, and *return statements*. \n",
    "\n",
    "After analyzing the function’s code, AutoGraph outputs an upgraded version of that function in which all the control flow statements are replaced by the appropriate TensorFlow operations, such as `tf.while_loop()` ,`for loops`, and `tf.cond()` for if statements. \n",
    "\n",
    "In the below figure, the function `sum_square(n)` is analyzed and transformed to the function `tf_sum_square(n)`, which is more convenient to be executed in the graph mode in the next step. When you invoke the former function: `sum_square(tf.constant(5))` for example, the upgraded function `tf__sum_squares()` function will be called with a symbolic tensor of type int32 and shape []. The function will run in graph mode, meaning that each TensorFlow operation will add a node in the graph to represent itself and its output tensor(s) (as opposed to the regular mode, called eager execution, or eager mode). In the following figure, you can see the `tf__sum_squares()` function being called with a symbolic tensor as its argument (in this case, an int32 tensor of shape []) and the final graph being generated during tracing. \n",
    "\n",
    "<img src='images/AutoGraph.png' align=center width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#0b486b\"> II.4. Placeholder Replacement and The Data API in TF 2.x </span>\n",
    "\n",
    "Tensorflow is designed to work with tensors, not numpy arrays. Therefore, we need data loading and processing operations to convert a raw data (i.e., text, images in digital form) to the tensor format. \n",
    "\n",
    "In TF 1.x we use a placeholder as a gate to feed data to train deep learning models. However, in TF 2.x, it is replaced by the Data API. Later in this unit, you will learn more about data processing operations, such as, batching, bufferring, transformation, etc (more detail in references [1,2] for curious learners). In this early introduction, we just show some basic data operations that replace placeholder. \n",
    "\n",
    "References:\n",
    "\n",
    "[1] Chapter 13 in *Hands-On Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems** by Geron Aurelien\n",
    "\n",
    "[2] Tensorflow tutorial in the link https://www.tensorflow.org/guide/data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "# Create a tensor with a numpy array\n",
    "a = np.array([2., 4., 5.]) # dtype is float64\n",
    "t = tf.constant(a) # dtype is float64\n",
    "\n",
    "print('a', a, type(a), a.dtype)\n",
    "print('t', t, type(t), t.dtype)\n",
    "\n",
    "# Convert a tensor back to a numpy array \n",
    "b = t.numpy() # dtype is float64\n",
    "print('b', b, type(b), b.dtype)\n",
    "\n",
    "# convert to a tensor with function \n",
    "# This function converts Python objects of various types to Tensor objects. \n",
    "# It accepts Tensor objects, numpy arrays, Python lists, and Python scalars.\n",
    "list_t = tf.convert_to_tensor([1.,2.,3.])\n",
    "print('list_t', list_t, type(list_t), list_t.dtype)\n",
    "\n",
    "# working with string \n",
    "s = 'Hello there'\n",
    "string_t = tf.constant(s)\n",
    "print('string_t', string_t, type(string_t), string_t.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that NumPy uses 64-bit precision by default, while TensorFlow uses 32-bit. This is because 32-bit precision is generally more than enough for neural networks, plus it runs faster and uses less RAM. So when you create a tensor from a NumPy array, make sure to set dtype=tf.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some functions can work with numpy array directly \n",
    "print(tf.square(a))\n",
    "\n",
    "# We can multiply a numpy array with a tensor \n",
    "print(tf.multiply(a, t))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### <span style=\"color:#0b486b\"> <div  style=\"text-align:center\">**THE END**</div> </span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.9 ('wk1': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "61a6c549c3e4371d87ce5bba127fa422dc046ff3e8cac899a57b1e0f997ec025"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
